{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Pharox Toolkit \u00b6 Pharox is the Python toolkit for building serious proxy orchestration systems. It focuses on the business rules\u2014leasing, storage contracts, health orchestration and lifecycle hooks\u2014so you can plug it into any application or service. Just want to try it? Head straight to the Quickstart for an end-to-end walkthrough you can run locally in under five minutes. Why Pharox \u00b6 Battle-tested leasing logic with concurrency caps, cleanup helpers and lifecycle callbacks ready for observability. Storage abstraction that scales : swap the in-memory adapter for your own datastore through the IStorage interface and shared contract tests. Health orchestration that aligns workers, services and SDKs behind the same HealthCheckResult semantics. Modern Python ergonomics : Pydantic v2 models, type hints, Ruff formatting, and context managers that reduce boilerplate. Choose Your Path \u00b6 If you want to\u2026 Start here Related reference Install, seed data, lease a proxy Quickstart ProxyManager Embed Pharox in a worker or script Embed Pharox in a Worker pharox.utils.bootstrap Wire Pharox to a SQL datastore Build a PostgreSQL Adapter IStorage contract Run protocol health sweeps Run Health Checks at Scale Health Toolkit Architecture Snapshot \u00b6 The toolkit sits between your code and the storage layer: Your automation, service or CLI drives ProxyManager . ProxyManager delegates persistence to an IStorage implementation. Health checks use HealthChecker / HealthCheckOrchestrator to enforce consistent classification. Callbacks and metrics hooks let you surface events without forking the core. Community & Support \u00b6 GitHub issues and discussions: https://github.com/fzaca/pharox PyPI releases: https://pypi.org/project/pharox/ Documentation source: this site, built with MkDocs Material\u2014PRs welcome!","title":"Home"},{"location":"#pharox-toolkit","text":"Pharox is the Python toolkit for building serious proxy orchestration systems. It focuses on the business rules\u2014leasing, storage contracts, health orchestration and lifecycle hooks\u2014so you can plug it into any application or service. Just want to try it? Head straight to the Quickstart for an end-to-end walkthrough you can run locally in under five minutes.","title":"Pharox Toolkit"},{"location":"#why-pharox","text":"Battle-tested leasing logic with concurrency caps, cleanup helpers and lifecycle callbacks ready for observability. Storage abstraction that scales : swap the in-memory adapter for your own datastore through the IStorage interface and shared contract tests. Health orchestration that aligns workers, services and SDKs behind the same HealthCheckResult semantics. Modern Python ergonomics : Pydantic v2 models, type hints, Ruff formatting, and context managers that reduce boilerplate.","title":"Why Pharox"},{"location":"#choose-your-path","text":"If you want to\u2026 Start here Related reference Install, seed data, lease a proxy Quickstart ProxyManager Embed Pharox in a worker or script Embed Pharox in a Worker pharox.utils.bootstrap Wire Pharox to a SQL datastore Build a PostgreSQL Adapter IStorage contract Run protocol health sweeps Run Health Checks at Scale Health Toolkit","title":"Choose Your Path"},{"location":"#architecture-snapshot","text":"The toolkit sits between your code and the storage layer: Your automation, service or CLI drives ProxyManager . ProxyManager delegates persistence to an IStorage implementation. Health checks use HealthChecker / HealthCheckOrchestrator to enforce consistent classification. Callbacks and metrics hooks let you surface events without forking the core.","title":"Architecture Snapshot"},{"location":"#community-support","text":"GitHub issues and discussions: https://github.com/fzaca/pharox PyPI releases: https://pypi.org/project/pharox/ Documentation source: this site, built with MkDocs Material\u2014PRs welcome!","title":"Community &amp; Support"},{"location":"health-checks/","text":"Health Checking Toolkit \u00b6 Every Pharox deployment needs a consistent, repeatable way to verify that proxies are reachable and responsive. The health module bundles that logic so scripts, SDK workers, and services all classify proxies the same way. Looking for a complete sweep pipeline? See Run Health Checks at Scale for an end-to-end example that streams results and persists them via IStorage . Key Building Blocks \u00b6 HealthCheckOptions ( pharox.models ): runtime configuration for a probe (target URL, attempts, timeout, expected status codes, latency threshold, request headers, redirect policy). HealthChecker ( pharox.health ): orchestrates checks, choosing the strategy that matches each proxy\u2019s protocol and returning a HealthCheckResult . HTTPHealthCheckStrategy : default strategy used for HTTP, HTTPS, SOCKS4, and SOCKS5 proxies. It performs real HTTP requests through the proxy and classifies the result as active , slow , or inactive . Note : SOCKS support relies on httpx[socks] . Install it alongside Pharox with pip install httpx[socks] if you plan to probe SOCKS4/5 endpoints. You can register additional strategies for custom protocols or handshake behaviour, while reusing the rest of the orchestration logic. Quick Example \u00b6 import asyncio from uuid import uuid4 from pharox import HealthCheckOptions, HealthChecker, Proxy, ProxyProtocol proxy = Proxy( host=\"dc.oxylabs.io\", port=8001, protocol=ProxyProtocol.HTTP, pool_id=uuid4(), # replace with an existing pool ID in your storage ) checker = HealthChecker() options = HealthCheckOptions( target_url=\"https://example.com/status/204\", expected_status_codes=[204], attempts=2, timeout=5.0, ) result = asyncio.run(checker.check_proxy(proxy, options=options)) print(result.status, result.latency_ms, result.status_code) Options Reference \u00b6 Field Purpose Tips target_url Endpoint hit through the proxy Use a low-latency endpoint under your control when possible. timeout Per-attempt timeout (seconds) Keep conservative defaults; health checks should fail fast. attempts Maximum retries Combine with expected_status_codes to tolerate transient errors. expected_status_codes Acceptable HTTP codes Include every success code returned by your target URL. slow_threshold_ms Latency boundary between active and slow Tune per workload. slow still indicates connectivity. headers Extra request headers Useful for provider auth tokens or tracing headers. allow_redirects Follow HTTP redirects Disable when you want to detect 3xx responses explicitly. Batch Checks with Streaming \u00b6 HealthChecker.stream_health_checks launches checks concurrently and yields results as they complete: from pharox import ProxyStatus async def sweep(proxies): checker = HealthChecker() async for result in checker.stream_health_checks(proxies): if result.status in {ProxyStatus.ACTIVE, ProxyStatus.SLOW}: handle_healthy_proxy(result) else: handle_unhealthy_proxy(result) asyncio.run(sweep(proxy_list)) This pattern is ideal for scheduled health workers or on-demand diagnostics. The async generator keeps memory usage predictable even when you probe large lists. Integrating with ProxyManager \u00b6 Health checks often precede or follow leasing operations: Acquire a candidate proxy using ProxyManager.acquire_proxy . Run a health check with the desired options. Release the lease if the proxy fails or exhibits high latency. from pharox import ProxyStatus async def acquire_and_probe(manager, storage, checker): lease = manager.acquire_proxy(pool_name=\"latam-residential\") if not lease: return None proxy = storage.get_proxy_by_id(lease.proxy_id) health = await checker.check_proxy(proxy) if health.status is ProxyStatus.INACTIVE: manager.release_proxy(lease) return None return lease The workflow mirrors the behaviour you\u2019ll deploy in the FastAPI service or SDK workers, ensuring both components make consistent decisions. Custom Strategies \u00b6 Some environments require protocol-specific probes (for example, negotiating a SOCKS5 authentication step or verifying a proprietary tunnel). Implement HealthCheckStrategy and register it for the relevant protocol: from pharox import ProxyProtocol from pharox.health import HealthCheckStrategy, HealthChecker class RedisTunnelStrategy(HealthCheckStrategy): async def check(self, proxy, options): ... # perform tunnel handshake and return HealthCheckResult checker = HealthChecker() checker.register_strategy(ProxyProtocol.SOCKS5, RedisTunnelStrategy()) Storing and Acting on Results \u00b6 HealthCheckResult captures: status ( ProxyStatus.ACTIVE , SLOW , or INACTIVE ) latency_ms (per successful attempt or timeout duration) attempts used before reaching a decision status_code and optional error_message checked_at timestamp (UTC) Persist these results in your own layer if you need historical analytics or automated remediation. The toolkit intentionally stops short of storing anything so it stays embeddable in any environment. Local Testing Helpers \u00b6 The repository includes drafts/run_proxy_health_checks.py , a small script that seeds InMemoryStorage , leases proxies, and runs simple or bulk health checks. Use it as a reference when wiring the toolkit to real providers or when debugging proxy credentials.","title":"Health Toolkit"},{"location":"health-checks/#health-checking-toolkit","text":"Every Pharox deployment needs a consistent, repeatable way to verify that proxies are reachable and responsive. The health module bundles that logic so scripts, SDK workers, and services all classify proxies the same way. Looking for a complete sweep pipeline? See Run Health Checks at Scale for an end-to-end example that streams results and persists them via IStorage .","title":"Health Checking Toolkit"},{"location":"health-checks/#key-building-blocks","text":"HealthCheckOptions ( pharox.models ): runtime configuration for a probe (target URL, attempts, timeout, expected status codes, latency threshold, request headers, redirect policy). HealthChecker ( pharox.health ): orchestrates checks, choosing the strategy that matches each proxy\u2019s protocol and returning a HealthCheckResult . HTTPHealthCheckStrategy : default strategy used for HTTP, HTTPS, SOCKS4, and SOCKS5 proxies. It performs real HTTP requests through the proxy and classifies the result as active , slow , or inactive . Note : SOCKS support relies on httpx[socks] . Install it alongside Pharox with pip install httpx[socks] if you plan to probe SOCKS4/5 endpoints. You can register additional strategies for custom protocols or handshake behaviour, while reusing the rest of the orchestration logic.","title":"Key Building Blocks"},{"location":"health-checks/#quick-example","text":"import asyncio from uuid import uuid4 from pharox import HealthCheckOptions, HealthChecker, Proxy, ProxyProtocol proxy = Proxy( host=\"dc.oxylabs.io\", port=8001, protocol=ProxyProtocol.HTTP, pool_id=uuid4(), # replace with an existing pool ID in your storage ) checker = HealthChecker() options = HealthCheckOptions( target_url=\"https://example.com/status/204\", expected_status_codes=[204], attempts=2, timeout=5.0, ) result = asyncio.run(checker.check_proxy(proxy, options=options)) print(result.status, result.latency_ms, result.status_code)","title":"Quick Example"},{"location":"health-checks/#options-reference","text":"Field Purpose Tips target_url Endpoint hit through the proxy Use a low-latency endpoint under your control when possible. timeout Per-attempt timeout (seconds) Keep conservative defaults; health checks should fail fast. attempts Maximum retries Combine with expected_status_codes to tolerate transient errors. expected_status_codes Acceptable HTTP codes Include every success code returned by your target URL. slow_threshold_ms Latency boundary between active and slow Tune per workload. slow still indicates connectivity. headers Extra request headers Useful for provider auth tokens or tracing headers. allow_redirects Follow HTTP redirects Disable when you want to detect 3xx responses explicitly.","title":"Options Reference"},{"location":"health-checks/#batch-checks-with-streaming","text":"HealthChecker.stream_health_checks launches checks concurrently and yields results as they complete: from pharox import ProxyStatus async def sweep(proxies): checker = HealthChecker() async for result in checker.stream_health_checks(proxies): if result.status in {ProxyStatus.ACTIVE, ProxyStatus.SLOW}: handle_healthy_proxy(result) else: handle_unhealthy_proxy(result) asyncio.run(sweep(proxy_list)) This pattern is ideal for scheduled health workers or on-demand diagnostics. The async generator keeps memory usage predictable even when you probe large lists.","title":"Batch Checks with Streaming"},{"location":"health-checks/#integrating-with-proxymanager","text":"Health checks often precede or follow leasing operations: Acquire a candidate proxy using ProxyManager.acquire_proxy . Run a health check with the desired options. Release the lease if the proxy fails or exhibits high latency. from pharox import ProxyStatus async def acquire_and_probe(manager, storage, checker): lease = manager.acquire_proxy(pool_name=\"latam-residential\") if not lease: return None proxy = storage.get_proxy_by_id(lease.proxy_id) health = await checker.check_proxy(proxy) if health.status is ProxyStatus.INACTIVE: manager.release_proxy(lease) return None return lease The workflow mirrors the behaviour you\u2019ll deploy in the FastAPI service or SDK workers, ensuring both components make consistent decisions.","title":"Integrating with ProxyManager"},{"location":"health-checks/#custom-strategies","text":"Some environments require protocol-specific probes (for example, negotiating a SOCKS5 authentication step or verifying a proprietary tunnel). Implement HealthCheckStrategy and register it for the relevant protocol: from pharox import ProxyProtocol from pharox.health import HealthCheckStrategy, HealthChecker class RedisTunnelStrategy(HealthCheckStrategy): async def check(self, proxy, options): ... # perform tunnel handshake and return HealthCheckResult checker = HealthChecker() checker.register_strategy(ProxyProtocol.SOCKS5, RedisTunnelStrategy())","title":"Custom Strategies"},{"location":"health-checks/#storing-and-acting-on-results","text":"HealthCheckResult captures: status ( ProxyStatus.ACTIVE , SLOW , or INACTIVE ) latency_ms (per successful attempt or timeout duration) attempts used before reaching a decision status_code and optional error_message checked_at timestamp (UTC) Persist these results in your own layer if you need historical analytics or automated remediation. The toolkit intentionally stops short of storing anything so it stays embeddable in any environment.","title":"Storing and Acting on Results"},{"location":"health-checks/#local-testing-helpers","text":"The repository includes drafts/run_proxy_health_checks.py , a small script that seeds InMemoryStorage , leases proxies, and runs simple or bulk health checks. Use it as a reference when wiring the toolkit to real providers or when debugging proxy credentials.","title":"Local Testing Helpers"},{"location":"models/","text":"Models & Filters \u00b6 The toolkit ships Pydantic models that describe proxies, leases, and filtering options. These classes are shared across the SDK and service, so treating them as your contract keeps every integration aligned. Proxy \u00b6 from pharox import Proxy, ProxyProtocol, ProxyStatus proxy = Proxy( host=\"186.33.123.10\", port=8080, protocol=ProxyProtocol.HTTP, pool_id=pool.id, status=ProxyStatus.ACTIVE, country=\"AR\", source=\"oxylabs\", ) print(proxy.url) # http://186.33.123.10:8080 Important fields: protocol accepts HTTP, HTTPS, SOCKS4, or SOCKS5. credentials can hold user/password pairs. The url property encodes them automatically for client libraries like httpx or requests . max_concurrency limits simultaneous leases. None means unlimited. current_leases is managed by the storage adapter; treat it as read-only. ProxyPool \u00b6 Pools group proxies with shared configuration. Use pool names to pick distinct provider buckets or geographic segments. from pharox import ProxyPool pool = ProxyPool(name=\"latam-residential\", description=\"Residential LatAm proxies\") Consumers \u00b6 A Consumer identifies the actor leasing proxies. This can be a worker name, a service, or any identifier meaningful to your system. from pharox import Consumer storage.add_consumer(Consumer(name=\"worker-1\")) Leases \u00b6 Lease records who is using a proxy and when the access expires. You usually interact with this through ProxyManager.acquire_proxy , but the data model is available if you need to persist or audit leases. lease.id lease.proxy_id lease.expires_at Proxy Filters \u00b6 ProxyFilters lets you express selection criteria. All fields are optional, and adapters decide how to evaluate them. from pharox import ProxyFilters filters = ProxyFilters( country=\"CO\", source=\"latam-provider\", asn=12345, latitude=4.7110, longitude=-74.0721, radius_km=50, ) Validation rules: Latitude and longitude must appear together. If radius_km is set, geolocation coordinates are required. Status Enums \u00b6 Both proxies and leases expose enums for their lifecycle. from pharox import ProxyStatus, LeaseStatus if result.status is ProxyStatus.SLOW: ... if lease.status is LeaseStatus.RELEASED: ... Stay consistent with these enums when writing storage adapters or APIs so that all Pharox components can reason about state transitions identically.","title":"Models & Filters"},{"location":"models/#models-filters","text":"The toolkit ships Pydantic models that describe proxies, leases, and filtering options. These classes are shared across the SDK and service, so treating them as your contract keeps every integration aligned.","title":"Models &amp; Filters"},{"location":"models/#proxy","text":"from pharox import Proxy, ProxyProtocol, ProxyStatus proxy = Proxy( host=\"186.33.123.10\", port=8080, protocol=ProxyProtocol.HTTP, pool_id=pool.id, status=ProxyStatus.ACTIVE, country=\"AR\", source=\"oxylabs\", ) print(proxy.url) # http://186.33.123.10:8080 Important fields: protocol accepts HTTP, HTTPS, SOCKS4, or SOCKS5. credentials can hold user/password pairs. The url property encodes them automatically for client libraries like httpx or requests . max_concurrency limits simultaneous leases. None means unlimited. current_leases is managed by the storage adapter; treat it as read-only.","title":"Proxy"},{"location":"models/#proxypool","text":"Pools group proxies with shared configuration. Use pool names to pick distinct provider buckets or geographic segments. from pharox import ProxyPool pool = ProxyPool(name=\"latam-residential\", description=\"Residential LatAm proxies\")","title":"ProxyPool"},{"location":"models/#consumers","text":"A Consumer identifies the actor leasing proxies. This can be a worker name, a service, or any identifier meaningful to your system. from pharox import Consumer storage.add_consumer(Consumer(name=\"worker-1\"))","title":"Consumers"},{"location":"models/#leases","text":"Lease records who is using a proxy and when the access expires. You usually interact with this through ProxyManager.acquire_proxy , but the data model is available if you need to persist or audit leases. lease.id lease.proxy_id lease.expires_at","title":"Leases"},{"location":"models/#proxy-filters","text":"ProxyFilters lets you express selection criteria. All fields are optional, and adapters decide how to evaluate them. from pharox import ProxyFilters filters = ProxyFilters( country=\"CO\", source=\"latam-provider\", asn=12345, latitude=4.7110, longitude=-74.0721, radius_km=50, ) Validation rules: Latitude and longitude must appear together. If radius_km is set, geolocation coordinates are required.","title":"Proxy Filters"},{"location":"models/#status-enums","text":"Both proxies and leases expose enums for their lifecycle. from pharox import ProxyStatus, LeaseStatus if result.status is ProxyStatus.SLOW: ... if lease.status is LeaseStatus.RELEASED: ... Stay consistent with these enums when writing storage adapters or APIs so that all Pharox components can reason about state transitions identically.","title":"Status Enums"},{"location":"proxy-manager/","text":"Proxy Manager \u00b6 ProxyManager is the high-level orchestration API for leasing and releasing proxies. It uses a storage backend that implements the IStorage interface to keep track of pools, proxies, consumers, and leases. Creating a Manager \u00b6 from pharox import InMemoryStorage, ProxyManager storage = InMemoryStorage() manager = ProxyManager(storage) The manager itself is stateless; all persistent data lives in the storage backend. You can use the bundled in-memory adapter for tests and scripts, or wire your own adapter for production. Leasing a Proxy \u00b6 lease = manager.acquire_proxy( pool_name=\"latam-residential\", consumer_name=\"worker-1\", duration_seconds=300, ) if lease: print(\"Leased proxy\", lease.proxy_id) else: print(\"No proxy available\") Key points: If consumer_name is omitted, the manager falls back to the default consumer ( ProxyManager.DEFAULT_CONSUMER_NAME ). Make sure that consumer exists in storage. duration_seconds defines when the lease expires. The storage adapter is responsible for releasing expired leases. The manager automatically calls cleanup_expired_leases() before trying to allocate a proxy, so stale leases do not block new requests. Releasing a Proxy \u00b6 if lease: manager.release_proxy(lease) Leases should be released as soon as the caller finishes using the proxy. The storage layer decrements current_leases and updates the lease status. Using the with_lease Context Manager \u00b6 To avoid manual try/finally blocks, ProxyManager exposes a context manager that automatically releases the lease when the block exits: with manager.with_lease( pool_name=\"latam-residential\", consumer_name=\"worker-1\", duration_seconds=120, ) as lease: if not lease: raise RuntimeError(\"No proxy available\") proxy = storage.get_proxy_by_id(lease.proxy_id) do_work(proxy) If acquisition fails, the context yields None so your code can decide whether to retry or fall back to another pool. When a lease is returned, the manager releases it even if exceptions occur inside the with block. Async Flows \u00b6 The manager itself is synchronous, but you can still consume it from async code by off-loading blocking calls to a worker thread. Pharox bundles helpers for this exact use case: import asyncio from pharox import ( acquire_proxy_async, release_proxy_async, with_lease_async, ) async def runner(manager): lease = await acquire_proxy_async( manager, pool_name=\"latam-residential\", consumer_name=\"worker-1\", ) if lease: await release_proxy_async(manager, lease) async def main(manager): async with with_lease_async( manager, pool_name=\"latam-residential\", ) as lease: if not lease: return # perform async work here asyncio.run(main(manager)) All three helpers use asyncio.to_thread so they remain compatible with any existing IStorage implementation without introducing a hard dependency on an async driver. Filtering Proxies \u00b6 Use ProxyFilters to target specific proxies. Filters apply metadata such as country, provider, or geolocation: from pharox import ProxyFilters filters = ProxyFilters(country=\"AR\", source=\"oxylabs\") lease = manager.acquire_proxy( pool_name=\"latam-residential\", consumer_name=\"worker-1\", filters=filters, ) If you need radius-based matching, include latitude , longitude , and radius_km . Storage adapters are in charge of interpreting these filters. Handling Concurrency Limits \u00b6 Each Proxy can define max_concurrency . The storage implementation checks the current lease count and prevents over-leasing. from pharox import Proxy, ProxyStatus proxy = Proxy( host=\"1.1.1.1\", port=8080, protocol=\"http\", pool_id=pool.id, status=ProxyStatus.ACTIVE, max_concurrency=2, ) If all slots are in use, acquire_proxy returns None and the caller can retry or pick another pool. Cleaning Up Expired Leases \u00b6 You can trigger cleanup manually when running background jobs: released = manager.cleanup_expired_leases() print(\"Expired leases released:\", released) Well-behaved storage adapters should also perform cleanup on their own cadence (e.g., cron job, background task, or database job). Lifecycle Callbacks \u00b6 Register callbacks to hook into acquisition and release events\u2014for example, to emit metrics or structured logs: from pharox import AcquireEventPayload, ReleaseEventPayload def on_acquire(event: AcquireEventPayload): outcome = \"acquired\" if event.lease else \"miss\" duration = event.duration_ms stats = event.pool_stats.model_dump() if event.pool_stats else {} print( f\"{event.consumer_name} {outcome} from {event.pool_name} \" f\"in {duration} ms \u2014 stats: {stats}\" ) def on_release(event: ReleaseEventPayload): duration = event.lease_duration_ms or 0 available = ( event.pool_stats.available_proxies if event.pool_stats else \"n/a\" ) print( f\"Released {event.lease.proxy_id} after {duration} ms \" f\"(available proxies: {available})\" ) manager.register_acquire_callback(on_acquire) manager.register_release_callback(on_release) Callbacks now receive structured payloads with high-resolution timestamps, operation duration, and a PoolStatsSnapshot . Use these fields to emit metrics, compute queueing time, or alert when pools trend toward exhaustion. The release hook fires only when a lease is successfully released.","title":"Proxy Manager Deep Dive"},{"location":"proxy-manager/#proxy-manager","text":"ProxyManager is the high-level orchestration API for leasing and releasing proxies. It uses a storage backend that implements the IStorage interface to keep track of pools, proxies, consumers, and leases.","title":"Proxy Manager"},{"location":"proxy-manager/#creating-a-manager","text":"from pharox import InMemoryStorage, ProxyManager storage = InMemoryStorage() manager = ProxyManager(storage) The manager itself is stateless; all persistent data lives in the storage backend. You can use the bundled in-memory adapter for tests and scripts, or wire your own adapter for production.","title":"Creating a Manager"},{"location":"proxy-manager/#leasing-a-proxy","text":"lease = manager.acquire_proxy( pool_name=\"latam-residential\", consumer_name=\"worker-1\", duration_seconds=300, ) if lease: print(\"Leased proxy\", lease.proxy_id) else: print(\"No proxy available\") Key points: If consumer_name is omitted, the manager falls back to the default consumer ( ProxyManager.DEFAULT_CONSUMER_NAME ). Make sure that consumer exists in storage. duration_seconds defines when the lease expires. The storage adapter is responsible for releasing expired leases. The manager automatically calls cleanup_expired_leases() before trying to allocate a proxy, so stale leases do not block new requests.","title":"Leasing a Proxy"},{"location":"proxy-manager/#releasing-a-proxy","text":"if lease: manager.release_proxy(lease) Leases should be released as soon as the caller finishes using the proxy. The storage layer decrements current_leases and updates the lease status.","title":"Releasing a Proxy"},{"location":"proxy-manager/#using-the-with_lease-context-manager","text":"To avoid manual try/finally blocks, ProxyManager exposes a context manager that automatically releases the lease when the block exits: with manager.with_lease( pool_name=\"latam-residential\", consumer_name=\"worker-1\", duration_seconds=120, ) as lease: if not lease: raise RuntimeError(\"No proxy available\") proxy = storage.get_proxy_by_id(lease.proxy_id) do_work(proxy) If acquisition fails, the context yields None so your code can decide whether to retry or fall back to another pool. When a lease is returned, the manager releases it even if exceptions occur inside the with block.","title":"Using the with_lease Context Manager"},{"location":"proxy-manager/#async-flows","text":"The manager itself is synchronous, but you can still consume it from async code by off-loading blocking calls to a worker thread. Pharox bundles helpers for this exact use case: import asyncio from pharox import ( acquire_proxy_async, release_proxy_async, with_lease_async, ) async def runner(manager): lease = await acquire_proxy_async( manager, pool_name=\"latam-residential\", consumer_name=\"worker-1\", ) if lease: await release_proxy_async(manager, lease) async def main(manager): async with with_lease_async( manager, pool_name=\"latam-residential\", ) as lease: if not lease: return # perform async work here asyncio.run(main(manager)) All three helpers use asyncio.to_thread so they remain compatible with any existing IStorage implementation without introducing a hard dependency on an async driver.","title":"Async Flows"},{"location":"proxy-manager/#filtering-proxies","text":"Use ProxyFilters to target specific proxies. Filters apply metadata such as country, provider, or geolocation: from pharox import ProxyFilters filters = ProxyFilters(country=\"AR\", source=\"oxylabs\") lease = manager.acquire_proxy( pool_name=\"latam-residential\", consumer_name=\"worker-1\", filters=filters, ) If you need radius-based matching, include latitude , longitude , and radius_km . Storage adapters are in charge of interpreting these filters.","title":"Filtering Proxies"},{"location":"proxy-manager/#handling-concurrency-limits","text":"Each Proxy can define max_concurrency . The storage implementation checks the current lease count and prevents over-leasing. from pharox import Proxy, ProxyStatus proxy = Proxy( host=\"1.1.1.1\", port=8080, protocol=\"http\", pool_id=pool.id, status=ProxyStatus.ACTIVE, max_concurrency=2, ) If all slots are in use, acquire_proxy returns None and the caller can retry or pick another pool.","title":"Handling Concurrency Limits"},{"location":"proxy-manager/#cleaning-up-expired-leases","text":"You can trigger cleanup manually when running background jobs: released = manager.cleanup_expired_leases() print(\"Expired leases released:\", released) Well-behaved storage adapters should also perform cleanup on their own cadence (e.g., cron job, background task, or database job).","title":"Cleaning Up Expired Leases"},{"location":"proxy-manager/#lifecycle-callbacks","text":"Register callbacks to hook into acquisition and release events\u2014for example, to emit metrics or structured logs: from pharox import AcquireEventPayload, ReleaseEventPayload def on_acquire(event: AcquireEventPayload): outcome = \"acquired\" if event.lease else \"miss\" duration = event.duration_ms stats = event.pool_stats.model_dump() if event.pool_stats else {} print( f\"{event.consumer_name} {outcome} from {event.pool_name} \" f\"in {duration} ms \u2014 stats: {stats}\" ) def on_release(event: ReleaseEventPayload): duration = event.lease_duration_ms or 0 available = ( event.pool_stats.available_proxies if event.pool_stats else \"n/a\" ) print( f\"Released {event.lease.proxy_id} after {duration} ms \" f\"(available proxies: {available})\" ) manager.register_acquire_callback(on_acquire) manager.register_release_callback(on_release) Callbacks now receive structured payloads with high-resolution timestamps, operation duration, and a PoolStatsSnapshot . Use these fields to emit metrics, compute queueing time, or alert when pools trend toward exhaustion. The release hook fires only when a lease is successfully released.","title":"Lifecycle Callbacks"},{"location":"storage/","text":"Storage Adapters \u00b6 The toolkit separates business rules from persistence using the IStorage interface. You can plug in custom adapters for your database while reusing the same acquisition logic across applications. Need a template? Start from examples/postgres/ (code, migrations, Docker) and follow the PostgreSQL adapter walkthrough to tailor it to your datastore. In-Memory Storage \u00b6 The bundled InMemoryStorage is ideal for tests and exploratory scripts. from pharox import ( InMemoryStorage, ProxyPool, ProxyProtocol, ProxyStatus, bootstrap_consumer, bootstrap_pool, bootstrap_proxy, ) storage = InMemoryStorage() bootstrap_consumer(storage, name=\"default\") pool = bootstrap_pool(storage, name=\"latam-residential\") bootstrap_proxy( storage, pool=pool, host=\"186.33.123.10\", port=8080, protocol=ProxyProtocol.HTTP, status=ProxyStatus.ACTIVE, ) Characteristics: Thread-safe through an internal RLock . Keeps data structures in Python dictionaries; nothing is persisted to disk. Provides helper methods ( add_pool , add_proxy , add_consumer ) to seed test data. Production adapters can expose similar helpers or rely on migrations. ProxyManager automatically calls ensure_consumer for the default consumer so first acquisitions can succeed without manual seeding. For custom consumers, the bootstrap_consumer , bootstrap_pool , and bootstrap_proxy helpers keep examples concise. Implementing IStorage \u00b6 Custom adapters live in your service or SDK codebase. They must implement: find_available_proxy(pool_name, filters) create_lease(proxy, consumer_name, duration_seconds) ensure_consumer(consumer_name) release_lease(lease) cleanup_expired_leases() get_pool_stats(pool_name) Typical responsibilities include: Translating ProxyFilters into database queries. Enforcing max_concurrency when creating leases. Persisting lease state changes and adjusting current_leases counters. Computing pool snapshots for callbacks/telemetry ( PoolStatsSnapshot ). Returning defensive copies of models so callers cannot mutate shared state. You can extend the models with extra fields (e.g., tags , datacenter ) as long as they round-trip through the adapter and the additional metadata remains optional for other consumers. apply_health_check_result Best Practices \u00b6 Adapters own the source of truth for proxy health. When implementing apply_health_check_result , ensure the method: Updates status and checked_at atomically so new leases never see stale state. Use SELECT ... FOR UPDATE or equivalent row locks in SQL backends. Stores the latest latency/error metadata your organisation tracks (e.g., round-trip time, HTTP code, failure reason) so future dashboards and hooks can consume it. Keep optional columns nullable for compatibility. Resets counters when a proxy recovers (e.g., clear error_message , decrement failure streaks) and consider pausing leases when repeated failures push the status to INACTIVE or BANNED . Ignores unknown proxies gracefully (return None ) to keep orchestrators resilient if a row was removed mid-sweep. Emits updated Proxy copies (as the interface expects) so callbacks receive the latest snapshot without mutating shared state. Document these behaviours in your adapter repo so operators know how health data propagates into leasing decisions. See examples/postgres/ for a concrete reference. PostgreSQL Adapter \u00b6 Pharox includes pharox.storage.postgres.PostgresStorage , a SQLAlchemy Core adapter that implements every IStorage method. 1. Install the extra \u00b6 pip install 'pharox[postgres]' # or poetry install --extras postgres 2. Provision PostgreSQL \u00b6 docker compose -f examples/postgres/docker-compose.yml up -d psql postgresql://pharox:pharox@localhost:5439/pharox \\ -f examples/postgres/migrations/0001_init.sql Use the SQL migrations as a starting point, then migrate them into your own Alembic/Flyway changelog before production. 3. Instantiate the adapter \u00b6 from sqlalchemy import create_engine from pharox.manager import ProxyManager from pharox.storage.postgres import PostgresStorage engine = create_engine(\"postgresql+psycopg://pharox:pharox@localhost:5439/pharox\") storage = PostgresStorage(engine=engine) manager = ProxyManager(storage=storage) PostgresStorage exposes the same API surface as the in-memory adapter, including filters, lease cleanup, pool stats, and apply_health_check_result . Extend pharox.storage.postgres.tables if you need custom metadata columns. 4. Run the adapter contract suite \u00b6 export PHAROX_TEST_POSTGRES_URL=\"postgresql+psycopg://pharox:pharox@localhost:5439/pharox\" poetry run pytest tests/test_storage_contract_postgres.py The test truncates the tables between runs and verifies the adapter matches the behaviour expected by ProxyManager . 5. Seed proxies for experiments \u00b6 drafts/run_postgres_health_checks.py reseeds a pool with demo proxies and runs health checks through the Postgres adapter. Update the host/port/credential constants to point at your own providers when experimenting locally. Storage Adapter Cookbook \u00b6 The checklist below distils recurring patterns from production adapters. Mix and match the recipes according to your datastore. *** End Patch 5. Observability Hooks \u00b6 Expose acquisition/release telemetry via ProxyManager callbacks: Record miss rates ( lease is None ) to detect exhausted pools. Report duration_ms and pool_stats.available_proxies to your metrics stack. Log lease_duration_ms to spot jobs that hold proxies for too long. See the lifecycle hook guide for code samples. Validate with Contract Tests \u00b6 Use pharox.tests.adapters.storage_contract_suite to verify that your adapter behaves like the in-memory reference. Provide fixtures that insert pools and proxies into your datastore, then run the suite inside your pytest harness. This catches subtle regressions (filters, concurrency, stats) before consumers rely on the adapter in production. Install extras The optional postgres extra bundles SQLAlchemy, psycopg, and Alembic: pip install 'pharox[postgres]' or poetry install --extras postgres . Planning Additional Adapters \u00b6 Pharox now ships a reference PostgreSQL adapter ( pharox.storage.postgres ) plus examples/migrations under examples/postgres/ . Future adapters might target: Other relational databases (MySQL, SQL Server) using SQLAlchemy or native drivers. Document stores (MongoDB) for flexible metadata. Distributed caches (Redis) when leases need to be tracked at high volume. Keep each adapter project-specific so the toolkit remains storage-agnostic. When multiple teams need the same implementation, consider publishing it as a standalone package that depends on pharox . Using the Built-in PostgreSQL Adapter \u00b6 Pharox includes pharox.storage.postgres.PostgresStorage , a SQLAlchemy Core adapter that implements every IStorage method. 1. Install the extra \u00b6 pip install 'pharox[postgres]' # or poetry install --extras postgres 2. Provision PostgreSQL \u00b6 docker compose -f examples/postgres/docker-compose.yml up -d psql postgresql://pharox:pharox@localhost:5439/pharox \\ -f examples/postgres/migrations/0001_init.sql Use the SQL migrations as a starting point, then migrate them into your own Alembic/Flyway changelog before production. 3. Instantiate the adapter \u00b6 from sqlalchemy import create_engine from pharox.manager import ProxyManager from pharox.storage.postgres import PostgresStorage engine = create_engine(\"postgresql+psycopg://pharox:pharox@localhost:5439/pharox\") storage = PostgresStorage(engine=engine) manager = ProxyManager(storage=storage) PostgresStorage exposes the same API surface as the in-memory adapter, including filters, lease cleanup, pool stats, and apply_health_check_result . Extend pharox.storage.postgres.tables if you need custom metadata columns. 4. Run the adapter contract suite \u00b6 export PHAROX_TEST_POSTGRES_URL=\"postgresql+psycopg://pharox:pharox@localhost:5439/pharox\" poetry run pytest tests/test_storage_contract_postgres.py The test truncates the tables between runs and verifies the adapter matches the behaviour expected by ProxyManager . 5. Seed proxies for experiments \u00b6 drafts/run_postgres_health_checks.py reseeds a pool with demo proxies and runs health checks through the Postgres adapter. Update the host/port/credential constants to point at your own providers when experimenting locally.","title":"Storage Adapters"},{"location":"storage/#storage-adapters","text":"The toolkit separates business rules from persistence using the IStorage interface. You can plug in custom adapters for your database while reusing the same acquisition logic across applications. Need a template? Start from examples/postgres/ (code, migrations, Docker) and follow the PostgreSQL adapter walkthrough to tailor it to your datastore.","title":"Storage Adapters"},{"location":"storage/#in-memory-storage","text":"The bundled InMemoryStorage is ideal for tests and exploratory scripts. from pharox import ( InMemoryStorage, ProxyPool, ProxyProtocol, ProxyStatus, bootstrap_consumer, bootstrap_pool, bootstrap_proxy, ) storage = InMemoryStorage() bootstrap_consumer(storage, name=\"default\") pool = bootstrap_pool(storage, name=\"latam-residential\") bootstrap_proxy( storage, pool=pool, host=\"186.33.123.10\", port=8080, protocol=ProxyProtocol.HTTP, status=ProxyStatus.ACTIVE, ) Characteristics: Thread-safe through an internal RLock . Keeps data structures in Python dictionaries; nothing is persisted to disk. Provides helper methods ( add_pool , add_proxy , add_consumer ) to seed test data. Production adapters can expose similar helpers or rely on migrations. ProxyManager automatically calls ensure_consumer for the default consumer so first acquisitions can succeed without manual seeding. For custom consumers, the bootstrap_consumer , bootstrap_pool , and bootstrap_proxy helpers keep examples concise.","title":"In-Memory Storage"},{"location":"storage/#implementing-istorage","text":"Custom adapters live in your service or SDK codebase. They must implement: find_available_proxy(pool_name, filters) create_lease(proxy, consumer_name, duration_seconds) ensure_consumer(consumer_name) release_lease(lease) cleanup_expired_leases() get_pool_stats(pool_name) Typical responsibilities include: Translating ProxyFilters into database queries. Enforcing max_concurrency when creating leases. Persisting lease state changes and adjusting current_leases counters. Computing pool snapshots for callbacks/telemetry ( PoolStatsSnapshot ). Returning defensive copies of models so callers cannot mutate shared state. You can extend the models with extra fields (e.g., tags , datacenter ) as long as they round-trip through the adapter and the additional metadata remains optional for other consumers.","title":"Implementing IStorage"},{"location":"storage/#apply_health_check_result-best-practices","text":"Adapters own the source of truth for proxy health. When implementing apply_health_check_result , ensure the method: Updates status and checked_at atomically so new leases never see stale state. Use SELECT ... FOR UPDATE or equivalent row locks in SQL backends. Stores the latest latency/error metadata your organisation tracks (e.g., round-trip time, HTTP code, failure reason) so future dashboards and hooks can consume it. Keep optional columns nullable for compatibility. Resets counters when a proxy recovers (e.g., clear error_message , decrement failure streaks) and consider pausing leases when repeated failures push the status to INACTIVE or BANNED . Ignores unknown proxies gracefully (return None ) to keep orchestrators resilient if a row was removed mid-sweep. Emits updated Proxy copies (as the interface expects) so callbacks receive the latest snapshot without mutating shared state. Document these behaviours in your adapter repo so operators know how health data propagates into leasing decisions. See examples/postgres/ for a concrete reference.","title":"apply_health_check_result Best Practices"},{"location":"storage/#postgresql-adapter","text":"Pharox includes pharox.storage.postgres.PostgresStorage , a SQLAlchemy Core adapter that implements every IStorage method.","title":"PostgreSQL Adapter"},{"location":"storage/#1-install-the-extra","text":"pip install 'pharox[postgres]' # or poetry install --extras postgres","title":"1. Install the extra"},{"location":"storage/#2-provision-postgresql","text":"docker compose -f examples/postgres/docker-compose.yml up -d psql postgresql://pharox:pharox@localhost:5439/pharox \\ -f examples/postgres/migrations/0001_init.sql Use the SQL migrations as a starting point, then migrate them into your own Alembic/Flyway changelog before production.","title":"2. Provision PostgreSQL"},{"location":"storage/#3-instantiate-the-adapter","text":"from sqlalchemy import create_engine from pharox.manager import ProxyManager from pharox.storage.postgres import PostgresStorage engine = create_engine(\"postgresql+psycopg://pharox:pharox@localhost:5439/pharox\") storage = PostgresStorage(engine=engine) manager = ProxyManager(storage=storage) PostgresStorage exposes the same API surface as the in-memory adapter, including filters, lease cleanup, pool stats, and apply_health_check_result . Extend pharox.storage.postgres.tables if you need custom metadata columns.","title":"3. Instantiate the adapter"},{"location":"storage/#4-run-the-adapter-contract-suite","text":"export PHAROX_TEST_POSTGRES_URL=\"postgresql+psycopg://pharox:pharox@localhost:5439/pharox\" poetry run pytest tests/test_storage_contract_postgres.py The test truncates the tables between runs and verifies the adapter matches the behaviour expected by ProxyManager .","title":"4. Run the adapter contract suite"},{"location":"storage/#5-seed-proxies-for-experiments","text":"drafts/run_postgres_health_checks.py reseeds a pool with demo proxies and runs health checks through the Postgres adapter. Update the host/port/credential constants to point at your own providers when experimenting locally.","title":"5. Seed proxies for experiments"},{"location":"storage/#storage-adapter-cookbook","text":"The checklist below distils recurring patterns from production adapters. Mix and match the recipes according to your datastore. *** End Patch","title":"Storage Adapter Cookbook"},{"location":"storage/#5-observability-hooks","text":"Expose acquisition/release telemetry via ProxyManager callbacks: Record miss rates ( lease is None ) to detect exhausted pools. Report duration_ms and pool_stats.available_proxies to your metrics stack. Log lease_duration_ms to spot jobs that hold proxies for too long. See the lifecycle hook guide for code samples.","title":"5. Observability Hooks"},{"location":"storage/#validate-with-contract-tests","text":"Use pharox.tests.adapters.storage_contract_suite to verify that your adapter behaves like the in-memory reference. Provide fixtures that insert pools and proxies into your datastore, then run the suite inside your pytest harness. This catches subtle regressions (filters, concurrency, stats) before consumers rely on the adapter in production. Install extras The optional postgres extra bundles SQLAlchemy, psycopg, and Alembic: pip install 'pharox[postgres]' or poetry install --extras postgres .","title":"Validate with Contract Tests"},{"location":"storage/#planning-additional-adapters","text":"Pharox now ships a reference PostgreSQL adapter ( pharox.storage.postgres ) plus examples/migrations under examples/postgres/ . Future adapters might target: Other relational databases (MySQL, SQL Server) using SQLAlchemy or native drivers. Document stores (MongoDB) for flexible metadata. Distributed caches (Redis) when leases need to be tracked at high volume. Keep each adapter project-specific so the toolkit remains storage-agnostic. When multiple teams need the same implementation, consider publishing it as a standalone package that depends on pharox .","title":"Planning Additional Adapters"},{"location":"storage/#using-the-built-in-postgresql-adapter","text":"Pharox includes pharox.storage.postgres.PostgresStorage , a SQLAlchemy Core adapter that implements every IStorage method.","title":"Using the Built-in PostgreSQL Adapter"},{"location":"storage/#1-install-the-extra_1","text":"pip install 'pharox[postgres]' # or poetry install --extras postgres","title":"1. Install the extra"},{"location":"storage/#2-provision-postgresql_1","text":"docker compose -f examples/postgres/docker-compose.yml up -d psql postgresql://pharox:pharox@localhost:5439/pharox \\ -f examples/postgres/migrations/0001_init.sql Use the SQL migrations as a starting point, then migrate them into your own Alembic/Flyway changelog before production.","title":"2. Provision PostgreSQL"},{"location":"storage/#3-instantiate-the-adapter_1","text":"from sqlalchemy import create_engine from pharox.manager import ProxyManager from pharox.storage.postgres import PostgresStorage engine = create_engine(\"postgresql+psycopg://pharox:pharox@localhost:5439/pharox\") storage = PostgresStorage(engine=engine) manager = ProxyManager(storage=storage) PostgresStorage exposes the same API surface as the in-memory adapter, including filters, lease cleanup, pool stats, and apply_health_check_result . Extend pharox.storage.postgres.tables if you need custom metadata columns.","title":"3. Instantiate the adapter"},{"location":"storage/#4-run-the-adapter-contract-suite_1","text":"export PHAROX_TEST_POSTGRES_URL=\"postgresql+psycopg://pharox:pharox@localhost:5439/pharox\" poetry run pytest tests/test_storage_contract_postgres.py The test truncates the tables between runs and verifies the adapter matches the behaviour expected by ProxyManager .","title":"4. Run the adapter contract suite"},{"location":"storage/#5-seed-proxies-for-experiments_1","text":"drafts/run_postgres_health_checks.py reseeds a pool with demo proxies and runs health checks through the Postgres adapter. Update the host/port/credential constants to point at your own providers when experimenting locally.","title":"5. Seed proxies for experiments"},{"location":"theme/","text":"Documentation Theme (mkdocs-shadcn) \u00b6 The Pharox docs use the mkdocs-shadcn theme. It applies the shadcn/ui design system on top of MkDocs so the site feels modern without custom CSS. Quick Start \u00b6 Install the theme alongside MkDocs: pip install mkdocs mkdocs-shadcn Then enable it in mkdocs.yml : theme: name: shadcn Supported Extensions \u00b6 The theme works with the standard Markdown extensions plus several pymdownx helpers. Pharox currently enables: admonition tables toc codehilite pymdownx.details pymdownx.highlight pymdownx.inlinehilite pymdownx.superfences pymdownx.tabbed Syntax colors come from the theme\u2019s bundled Pygments palette ( github-dark ), configured through the pygments_style setting in mkdocs.yml . Additional extensions you can enable: pymdownx.blocks.details pymdownx.blocks.tab pymdownx.progressbar pymdownx.arithmatex built-in shadcn.echarts , shadcn.iconify , shadcn.codexec Plugins \u00b6 built-in excalidraw \u2013 edit diagrams in dev mode and render SVG at build time. mkdocstrings \u2013 auto-generate API docs from docstrings (experimental in the shadcn theme). These plugins are optional. Enable them in mkdocs.yml when you need the functionality. Developing the Theme \u00b6 The upstream project exposes its Tailwind CSS source for contributors. To work on the theme itself, clone the repository and install both the Python and CSS prerequisites: git clone https://github.com/asiffer/mkdocs-shadcn cd mkdocs-shadcn uv sync --all-extras bun install # or npm/yarn/pnpm Run the example docs in watch mode while hacking on the theme: cd pages/ uv run mkdocs serve --watch-theme -w .. In another terminal, keep the Tailwind watcher running from the project root: bun dev The Pharox repository does not need Tailwind or the dev tooling\u2014only the published mkdocs-shadcn package is required.","title":"Documentation Theme (mkdocs-shadcn)"},{"location":"theme/#documentation-theme-mkdocs-shadcn","text":"The Pharox docs use the mkdocs-shadcn theme. It applies the shadcn/ui design system on top of MkDocs so the site feels modern without custom CSS.","title":"Documentation Theme (mkdocs-shadcn)"},{"location":"theme/#quick-start","text":"Install the theme alongside MkDocs: pip install mkdocs mkdocs-shadcn Then enable it in mkdocs.yml : theme: name: shadcn","title":"Quick Start"},{"location":"theme/#supported-extensions","text":"The theme works with the standard Markdown extensions plus several pymdownx helpers. Pharox currently enables: admonition tables toc codehilite pymdownx.details pymdownx.highlight pymdownx.inlinehilite pymdownx.superfences pymdownx.tabbed Syntax colors come from the theme\u2019s bundled Pygments palette ( github-dark ), configured through the pygments_style setting in mkdocs.yml . Additional extensions you can enable: pymdownx.blocks.details pymdownx.blocks.tab pymdownx.progressbar pymdownx.arithmatex built-in shadcn.echarts , shadcn.iconify , shadcn.codexec","title":"Supported Extensions"},{"location":"theme/#plugins","text":"built-in excalidraw \u2013 edit diagrams in dev mode and render SVG at build time. mkdocstrings \u2013 auto-generate API docs from docstrings (experimental in the shadcn theme). These plugins are optional. Enable them in mkdocs.yml when you need the functionality.","title":"Plugins"},{"location":"theme/#developing-the-theme","text":"The upstream project exposes its Tailwind CSS source for contributors. To work on the theme itself, clone the repository and install both the Python and CSS prerequisites: git clone https://github.com/asiffer/mkdocs-shadcn cd mkdocs-shadcn uv sync --all-extras bun install # or npm/yarn/pnpm Run the example docs in watch mode while hacking on the theme: cd pages/ uv run mkdocs serve --watch-theme -w .. In another terminal, keep the Tailwind watcher running from the project root: bun dev The Pharox repository does not need Tailwind or the dev tooling\u2014only the published mkdocs-shadcn package is required.","title":"Developing the Theme"},{"location":"getting-started/quickstart/","text":"Quickstart \u00b6 This tutorial walks you through installing Pharox, seeding a proxy pool, leasing a proxy, and releasing it safely. All code runs locally with the bundled InMemoryStorage , so you do not need any external services. 1. Install Pharox \u00b6 Create and activate a virtual environment (optional but recommended), then install the package from PyPI: pip install pharox Python version Pharox targets Python 3.10+ and is fully type-hinted. If you are using pyenv or uv, make sure your interpreter is at least 3.10. 2. Bootstrap Storage and Manager \u00b6 Initialize the storage adapter and ProxyManager . The manager is stateless and delegates persistence to the storage backend. from pharox import ( InMemoryStorage, ProxyManager, ProxyProtocol, ProxyStatus, bootstrap_pool, bootstrap_proxy, ) storage = InMemoryStorage() manager = ProxyManager(storage=storage) 3. Seed a Pool and Proxy \u00b6 Use the bootstrap helpers to register a pool and proxy. When no consumer name is provided, the manager auto-registers the default consumer. The helper returns the persisted Proxy so you can reuse it later in health checks. pool = bootstrap_pool(storage, name=\"latam-residential\") seed_proxy = bootstrap_proxy( storage, pool=pool, host=\"1.1.1.1\", port=8080, protocol=ProxyProtocol.HTTP, status=ProxyStatus.ACTIVE, ) 4. Lease and Release a Proxy \u00b6 Lease a proxy from the pool. Prefer ProxyManager.with_lease \u2014the context manager guarantees cleanup even if the work inside the block raises an exception, and it accepts the same options as acquire_proxy (consumer name, filters, custom durations). with manager.with_lease(pool_name=pool.name, duration_seconds=60) as lease: if not lease: raise RuntimeError(\"No proxy available\") proxy = storage.get_proxy_by_id(lease.proxy_id) print(\"Leased proxy:\", proxy.url) # Do useful work with the proxy here... Behind the scenes Pharox: Ensures the consumer exists in storage. Cleans up expired leases to avoid blocking future acquisitions. Finds and locks an eligible proxy. Releases the lease automatically when the context closes. 5. Emit Metrics via Callbacks \u00b6 ProxyManager can publish acquisition/release events so you can tie into your observability stack without forking the toolkit. Register callbacks once after creating the manager (in the snippet below, metrics_client represents whatever telemetry helper\u2014StatsD, Prometheus, OTEL\u2014you already use): from pharox import AcquireEventPayload, ReleaseEventPayload def on_acquire(event: AcquireEventPayload): tags = { \"pool\": event.pool_name, \"consumer\": event.consumer_name, \"status\": \"hit\" if event.lease else \"miss\", } duration = event.duration_ms or 0 pool_stats = event.pool_stats.model_dump() if event.pool_stats else {} metrics_client.timing(\"pharox.acquire\", duration, tags) metrics_client.gauge(\"pharox.pool.available\", pool_stats.get(\"available_proxies\", 0), tags) def on_release(event: ReleaseEventPayload): duration = event.lease_duration_ms or 0 tags = {\"pool\": event.lease.pool_name} metrics_client.timing(\"pharox.lease_duration\", duration, tags) manager.register_acquire_callback(on_acquire) manager.register_release_callback(on_release) The payloads include timestamps, pool stats, and latency numbers that map directly to metrics or structured logs. Callbacks fire for every acquisition attempt, including misses, so you can track saturation and alert early. 6. Add Filters (Optional) \u00b6 Target proxies by metadata or location using ProxyFilters . from pharox import ProxyFilters filters = ProxyFilters(country=\"AR\", source=\"oxylabs\") lease = manager.acquire_proxy(pool_name=pool.name, filters=filters) if lease: print(\"Filtered lease:\", lease.proxy_id) manager.release_proxy(lease) 7. Run a Health Check \u00b6 Verify the proxy before using it in production. The health module classifies results consistently across all Pharox integrations. import asyncio from pharox import HealthCheckOptions, HealthChecker checker = HealthChecker() options = HealthCheckOptions( target_url=\"https://example.com/status/204\", expected_status_codes=[204], attempts=2, timeout=5.0, ) result = asyncio.run(checker.check_proxy(seed_proxy, options=options)) print(result.status, result.latency_ms) SOCKS proxies Install httpx[socks] alongside Pharox if you plan to probe SOCKS4/5 proxies: pip install \"httpx[socks]\" . Next Steps \u00b6 Dive deeper into ProxyManager and the lifecycle callbacks. Explore the lifecycle hook recipes to wire metrics and logs without forking the toolkit. Learn how to embed Pharox in a worker that runs inside a scheduler or automation pipeline. Explore the PostgreSQL adapter guide to store leases and health metrics persistently.","title":"Quickstart"},{"location":"getting-started/quickstart/#quickstart","text":"This tutorial walks you through installing Pharox, seeding a proxy pool, leasing a proxy, and releasing it safely. All code runs locally with the bundled InMemoryStorage , so you do not need any external services.","title":"Quickstart"},{"location":"getting-started/quickstart/#1-install-pharox","text":"Create and activate a virtual environment (optional but recommended), then install the package from PyPI: pip install pharox Python version Pharox targets Python 3.10+ and is fully type-hinted. If you are using pyenv or uv, make sure your interpreter is at least 3.10.","title":"1. Install Pharox"},{"location":"getting-started/quickstart/#2-bootstrap-storage-and-manager","text":"Initialize the storage adapter and ProxyManager . The manager is stateless and delegates persistence to the storage backend. from pharox import ( InMemoryStorage, ProxyManager, ProxyProtocol, ProxyStatus, bootstrap_pool, bootstrap_proxy, ) storage = InMemoryStorage() manager = ProxyManager(storage=storage)","title":"2. Bootstrap Storage and Manager"},{"location":"getting-started/quickstart/#3-seed-a-pool-and-proxy","text":"Use the bootstrap helpers to register a pool and proxy. When no consumer name is provided, the manager auto-registers the default consumer. The helper returns the persisted Proxy so you can reuse it later in health checks. pool = bootstrap_pool(storage, name=\"latam-residential\") seed_proxy = bootstrap_proxy( storage, pool=pool, host=\"1.1.1.1\", port=8080, protocol=ProxyProtocol.HTTP, status=ProxyStatus.ACTIVE, )","title":"3. Seed a Pool and Proxy"},{"location":"getting-started/quickstart/#4-lease-and-release-a-proxy","text":"Lease a proxy from the pool. Prefer ProxyManager.with_lease \u2014the context manager guarantees cleanup even if the work inside the block raises an exception, and it accepts the same options as acquire_proxy (consumer name, filters, custom durations). with manager.with_lease(pool_name=pool.name, duration_seconds=60) as lease: if not lease: raise RuntimeError(\"No proxy available\") proxy = storage.get_proxy_by_id(lease.proxy_id) print(\"Leased proxy:\", proxy.url) # Do useful work with the proxy here... Behind the scenes Pharox: Ensures the consumer exists in storage. Cleans up expired leases to avoid blocking future acquisitions. Finds and locks an eligible proxy. Releases the lease automatically when the context closes.","title":"4. Lease and Release a Proxy"},{"location":"getting-started/quickstart/#5-emit-metrics-via-callbacks","text":"ProxyManager can publish acquisition/release events so you can tie into your observability stack without forking the toolkit. Register callbacks once after creating the manager (in the snippet below, metrics_client represents whatever telemetry helper\u2014StatsD, Prometheus, OTEL\u2014you already use): from pharox import AcquireEventPayload, ReleaseEventPayload def on_acquire(event: AcquireEventPayload): tags = { \"pool\": event.pool_name, \"consumer\": event.consumer_name, \"status\": \"hit\" if event.lease else \"miss\", } duration = event.duration_ms or 0 pool_stats = event.pool_stats.model_dump() if event.pool_stats else {} metrics_client.timing(\"pharox.acquire\", duration, tags) metrics_client.gauge(\"pharox.pool.available\", pool_stats.get(\"available_proxies\", 0), tags) def on_release(event: ReleaseEventPayload): duration = event.lease_duration_ms or 0 tags = {\"pool\": event.lease.pool_name} metrics_client.timing(\"pharox.lease_duration\", duration, tags) manager.register_acquire_callback(on_acquire) manager.register_release_callback(on_release) The payloads include timestamps, pool stats, and latency numbers that map directly to metrics or structured logs. Callbacks fire for every acquisition attempt, including misses, so you can track saturation and alert early.","title":"5. Emit Metrics via Callbacks"},{"location":"getting-started/quickstart/#6-add-filters-optional","text":"Target proxies by metadata or location using ProxyFilters . from pharox import ProxyFilters filters = ProxyFilters(country=\"AR\", source=\"oxylabs\") lease = manager.acquire_proxy(pool_name=pool.name, filters=filters) if lease: print(\"Filtered lease:\", lease.proxy_id) manager.release_proxy(lease)","title":"6. Add Filters (Optional)"},{"location":"getting-started/quickstart/#7-run-a-health-check","text":"Verify the proxy before using it in production. The health module classifies results consistently across all Pharox integrations. import asyncio from pharox import HealthCheckOptions, HealthChecker checker = HealthChecker() options = HealthCheckOptions( target_url=\"https://example.com/status/204\", expected_status_codes=[204], attempts=2, timeout=5.0, ) result = asyncio.run(checker.check_proxy(seed_proxy, options=options)) print(result.status, result.latency_ms) SOCKS proxies Install httpx[socks] alongside Pharox if you plan to probe SOCKS4/5 proxies: pip install \"httpx[socks]\" .","title":"7. Run a Health Check"},{"location":"getting-started/quickstart/#next-steps","text":"Dive deeper into ProxyManager and the lifecycle callbacks. Explore the lifecycle hook recipes to wire metrics and logs without forking the toolkit. Learn how to embed Pharox in a worker that runs inside a scheduler or automation pipeline. Explore the PostgreSQL adapter guide to store leases and health metrics persistently.","title":"Next Steps"},{"location":"how-to/embed-worker/","text":"Embed Pharox in a Worker \u00b6 This guide shows how to integrate Pharox into a long-running worker that leases proxies, performs work, and emits observability events. Scenario You operate a scraping worker scheduled by Celery or a cron job. Each job needs an exclusive proxy, must release it after use, and should record metrics about lease success/failure. 1. Set Up the Manager and Callbacks \u00b6 Define callbacks once at process startup to centralise logging/metrics. import logging from pharox import ( AcquireEventPayload, InMemoryStorage, ProxyManager, ReleaseEventPayload, ) logger = logging.getLogger(\"pharox.worker\") storage = InMemoryStorage() manager = ProxyManager(storage=storage) def on_acquire(event: AcquireEventPayload): outcome = \"success\" if event.lease else \"failure\" logger.info( \"proxy.acquire\", extra={ \"pool\": event.pool_name, \"consumer\": event.consumer_name, \"outcome\": outcome, \"duration_ms\": event.duration_ms, \"filters\": event.filters.model_dump() if event.filters else None, \"available\": event.pool_stats.available_proxies if event.pool_stats else None, }, ) def on_release(event: ReleaseEventPayload): logger.info( \"proxy.release\", extra={ \"proxy_id\": event.lease.proxy_id, \"lease_duration_ms\": event.lease_duration_ms, \"available\": event.pool_stats.available_proxies if event.pool_stats else None, }, ) manager.register_acquire_callback(on_acquire) manager.register_release_callback(on_release) Tip Swap logging for your telemetry stack (OpenTelemetry, StatsD, Prometheus) by pushing the same metadata to counters/histograms. 2. Write a Worker Function \u00b6 Wrap the work in the with_lease context manager. Lease failures return None , allowing you to requeue or back off gracefully. def process_account(account_id: str) -> None: with manager.with_lease(pool_name=\"residential\", consumer_name=\"worker\") as lease: if not lease: logger.warning(\"No proxy available; retrying account %s\", account_id) raise RuntimeError(\"proxy unavailable\") proxy = storage.get_proxy_by_id(lease.proxy_id) run_job(account_id, proxy.url) Use your orchestration tool (Celery, RQ, APScheduler) to call process_account with retry policies that fit your workload. 3. Handle Errors Safely \u00b6 Because the context manager releases the lease in a finally block, any raised exceptions do not leak the proxy. If you need custom recovery logic, use a try/except inside the context: with manager.with_lease(pool_name=\"residential\") as lease: if not lease: return try: run_job(...) except ProviderTimeout as exc: logger.exception(\"Work failed, marking proxy as suspect\", exc_info=exc) # Optionally adjust proxy state in storage here. 4. Periodic Cleanup and Health Checks \u00b6 Schedule a background job that releases expired leases and runs health sweeps: import asyncio from pharox import HealthCheckOrchestrator checker = HealthCheckOrchestrator(storage=storage) async def sweep_proxies(): proxies = storage.list_proxies(status=\"active\") async for result in checker.stream_health_checks(proxies): logger.info( \"proxy.health\", extra={ \"proxy_id\": result.proxy_id, \"status\": result.status.value, \"latency_ms\": result.latency_ms, }, ) def nightly_maintenance(): released = manager.cleanup_expired_leases() logger.info(\"Expired leases cleaned\", extra={\"released\": released}) asyncio.run(sweep_proxies()) Tie nightly_maintenance to a cron trigger or a lightweight scheduler. 5. Promote to External Storage \u00b6 When you are ready for persistence, implement the IStorage interface or follow the PostgreSQL adapter walkthrough . The worker code above continues to work unchanged once the storage backend is swapped.","title":"Embed Pharox in a Worker"},{"location":"how-to/embed-worker/#embed-pharox-in-a-worker","text":"This guide shows how to integrate Pharox into a long-running worker that leases proxies, performs work, and emits observability events. Scenario You operate a scraping worker scheduled by Celery or a cron job. Each job needs an exclusive proxy, must release it after use, and should record metrics about lease success/failure.","title":"Embed Pharox in a Worker"},{"location":"how-to/embed-worker/#1-set-up-the-manager-and-callbacks","text":"Define callbacks once at process startup to centralise logging/metrics. import logging from pharox import ( AcquireEventPayload, InMemoryStorage, ProxyManager, ReleaseEventPayload, ) logger = logging.getLogger(\"pharox.worker\") storage = InMemoryStorage() manager = ProxyManager(storage=storage) def on_acquire(event: AcquireEventPayload): outcome = \"success\" if event.lease else \"failure\" logger.info( \"proxy.acquire\", extra={ \"pool\": event.pool_name, \"consumer\": event.consumer_name, \"outcome\": outcome, \"duration_ms\": event.duration_ms, \"filters\": event.filters.model_dump() if event.filters else None, \"available\": event.pool_stats.available_proxies if event.pool_stats else None, }, ) def on_release(event: ReleaseEventPayload): logger.info( \"proxy.release\", extra={ \"proxy_id\": event.lease.proxy_id, \"lease_duration_ms\": event.lease_duration_ms, \"available\": event.pool_stats.available_proxies if event.pool_stats else None, }, ) manager.register_acquire_callback(on_acquire) manager.register_release_callback(on_release) Tip Swap logging for your telemetry stack (OpenTelemetry, StatsD, Prometheus) by pushing the same metadata to counters/histograms.","title":"1. Set Up the Manager and Callbacks"},{"location":"how-to/embed-worker/#2-write-a-worker-function","text":"Wrap the work in the with_lease context manager. Lease failures return None , allowing you to requeue or back off gracefully. def process_account(account_id: str) -> None: with manager.with_lease(pool_name=\"residential\", consumer_name=\"worker\") as lease: if not lease: logger.warning(\"No proxy available; retrying account %s\", account_id) raise RuntimeError(\"proxy unavailable\") proxy = storage.get_proxy_by_id(lease.proxy_id) run_job(account_id, proxy.url) Use your orchestration tool (Celery, RQ, APScheduler) to call process_account with retry policies that fit your workload.","title":"2. Write a Worker Function"},{"location":"how-to/embed-worker/#3-handle-errors-safely","text":"Because the context manager releases the lease in a finally block, any raised exceptions do not leak the proxy. If you need custom recovery logic, use a try/except inside the context: with manager.with_lease(pool_name=\"residential\") as lease: if not lease: return try: run_job(...) except ProviderTimeout as exc: logger.exception(\"Work failed, marking proxy as suspect\", exc_info=exc) # Optionally adjust proxy state in storage here.","title":"3. Handle Errors Safely"},{"location":"how-to/embed-worker/#4-periodic-cleanup-and-health-checks","text":"Schedule a background job that releases expired leases and runs health sweeps: import asyncio from pharox import HealthCheckOrchestrator checker = HealthCheckOrchestrator(storage=storage) async def sweep_proxies(): proxies = storage.list_proxies(status=\"active\") async for result in checker.stream_health_checks(proxies): logger.info( \"proxy.health\", extra={ \"proxy_id\": result.proxy_id, \"status\": result.status.value, \"latency_ms\": result.latency_ms, }, ) def nightly_maintenance(): released = manager.cleanup_expired_leases() logger.info(\"Expired leases cleaned\", extra={\"released\": released}) asyncio.run(sweep_proxies()) Tie nightly_maintenance to a cron trigger or a lightweight scheduler.","title":"4. Periodic Cleanup and Health Checks"},{"location":"how-to/embed-worker/#5-promote-to-external-storage","text":"When you are ready for persistence, implement the IStorage interface or follow the PostgreSQL adapter walkthrough . The worker code above continues to work unchanged once the storage backend is swapped.","title":"5. Promote to External Storage"},{"location":"how-to/health-sweeps/","text":"Run Health Checks at Scale \u00b6 Large proxy pools need consistent, protocol-aware health checks. Pharox provides HealthChecker for lightweight probes and HealthCheckOrchestrator when you want to persist results via IStorage.apply_health_check_result . 1. Pick the Right Entry Point \u00b6 Use case Entry point Ad-hoc validation before leasing HealthChecker.check_proxy Batch sweep with custom storage handling HealthChecker.stream_health_checks Batch sweep that should update storage automatically HealthCheckOrchestrator.stream_health_checks 2. Configure Options Per Protocol \u00b6 Define defaults and overrides to account for latency differences or HTTP codes. from pharox import HealthCheckOptions, HealthChecker, ProxyProtocol checker = HealthChecker( default_options=HealthCheckOptions( target_url=\"https://example.com/status/204\", expected_status_codes=[204], timeout=5.0, attempts=2, slow_threshold_ms=1500, ) ) checker.set_protocol_options( ProxyProtocol.SOCKS5, HealthCheckOptions( target_url=\"https://example.com/ping\", expected_status_codes=[200], timeout=8.0, attempts=3, slow_threshold_ms=2500, ), ) 3. Stream Results \u00b6 Feed an iterable of Proxy objects. Results arrive as soon as each awaitable completes. import asyncio from pharox import ProxyStatus async def sweep(proxies): async for result in checker.stream_health_checks(proxies): if result.status in {ProxyStatus.ACTIVE, ProxyStatus.SLOW}: record_success(result) else: quarantine(result) active_proxies = my_storage.load_active_proxies() # implement in your adapter layer asyncio.run(sweep(active_proxies)) 4. Persist Outcomes Automatically \u00b6 Use the orchestrator to apply results via the storage adapter. from pharox import HealthCheckOrchestrator orchestrator = HealthCheckOrchestrator(storage=my_storage, checker=checker) async def sweep_and_update(): proxies = my_storage.load_proxies_for_healthchecks() async for result in orchestrator.stream_health_checks(proxies): metrics_client.record_latency(result.proxy_id, result.latency_ms) asyncio.run(sweep_and_update()) Storage responsibilities apply_health_check_result is where health data becomes authoritative. Follow the best practices : update status + checked_at atomically, persist latency/error metadata, and return the refreshed Proxy so callbacks receive the latest snapshot. 5. Coordinate with Leasing \u00b6 Health sweeps often run alongside leasing activity. Recommended pattern: Run manager.cleanup_expired_leases() before a sweep to free stale locks. Pause sweeps during peak acquisition bursts, or throttle concurrency. Use callbacks to emit events when a lease is skipped due to health changes. 6. Visualise Results \u00b6 Feed results into Prometheus/Grafana dashboards (latency histograms, error counts per provider). Store history in a time-series database for trend analysis. Trigger alerts when ProxyStatus.INACTIVE exceeds thresholds in a pool. For a hands-on example, check the drafts/run_proxy_health_checks.py script in the repository or create your own under examples/health-sweeps/ .","title":"Run Health Checks at Scale"},{"location":"how-to/health-sweeps/#run-health-checks-at-scale","text":"Large proxy pools need consistent, protocol-aware health checks. Pharox provides HealthChecker for lightweight probes and HealthCheckOrchestrator when you want to persist results via IStorage.apply_health_check_result .","title":"Run Health Checks at Scale"},{"location":"how-to/health-sweeps/#1-pick-the-right-entry-point","text":"Use case Entry point Ad-hoc validation before leasing HealthChecker.check_proxy Batch sweep with custom storage handling HealthChecker.stream_health_checks Batch sweep that should update storage automatically HealthCheckOrchestrator.stream_health_checks","title":"1. Pick the Right Entry Point"},{"location":"how-to/health-sweeps/#2-configure-options-per-protocol","text":"Define defaults and overrides to account for latency differences or HTTP codes. from pharox import HealthCheckOptions, HealthChecker, ProxyProtocol checker = HealthChecker( default_options=HealthCheckOptions( target_url=\"https://example.com/status/204\", expected_status_codes=[204], timeout=5.0, attempts=2, slow_threshold_ms=1500, ) ) checker.set_protocol_options( ProxyProtocol.SOCKS5, HealthCheckOptions( target_url=\"https://example.com/ping\", expected_status_codes=[200], timeout=8.0, attempts=3, slow_threshold_ms=2500, ), )","title":"2. Configure Options Per Protocol"},{"location":"how-to/health-sweeps/#3-stream-results","text":"Feed an iterable of Proxy objects. Results arrive as soon as each awaitable completes. import asyncio from pharox import ProxyStatus async def sweep(proxies): async for result in checker.stream_health_checks(proxies): if result.status in {ProxyStatus.ACTIVE, ProxyStatus.SLOW}: record_success(result) else: quarantine(result) active_proxies = my_storage.load_active_proxies() # implement in your adapter layer asyncio.run(sweep(active_proxies))","title":"3. Stream Results"},{"location":"how-to/health-sweeps/#4-persist-outcomes-automatically","text":"Use the orchestrator to apply results via the storage adapter. from pharox import HealthCheckOrchestrator orchestrator = HealthCheckOrchestrator(storage=my_storage, checker=checker) async def sweep_and_update(): proxies = my_storage.load_proxies_for_healthchecks() async for result in orchestrator.stream_health_checks(proxies): metrics_client.record_latency(result.proxy_id, result.latency_ms) asyncio.run(sweep_and_update()) Storage responsibilities apply_health_check_result is where health data becomes authoritative. Follow the best practices : update status + checked_at atomically, persist latency/error metadata, and return the refreshed Proxy so callbacks receive the latest snapshot.","title":"4. Persist Outcomes Automatically"},{"location":"how-to/health-sweeps/#5-coordinate-with-leasing","text":"Health sweeps often run alongside leasing activity. Recommended pattern: Run manager.cleanup_expired_leases() before a sweep to free stale locks. Pause sweeps during peak acquisition bursts, or throttle concurrency. Use callbacks to emit events when a lease is skipped due to health changes.","title":"5. Coordinate with Leasing"},{"location":"how-to/health-sweeps/#6-visualise-results","text":"Feed results into Prometheus/Grafana dashboards (latency histograms, error counts per provider). Store history in a time-series database for trend analysis. Trigger alerts when ProxyStatus.INACTIVE exceeds thresholds in a pool. For a hands-on example, check the drafts/run_proxy_health_checks.py script in the repository or create your own under examples/health-sweeps/ .","title":"6. Visualise Results"},{"location":"how-to/lifecycle-hooks/","text":"Instrument Lifecycle Hooks \u00b6 ProxyManager emits structured events every time it acquires or releases a lease. This guide shows how to connect those callbacks to your observability stack so you can track saturation, latency, and pool health without forking the toolkit. 1. Know the Payloads \u00b6 Callbacks receive Pydantic models that are already validated and timezone-aware: Field AcquireEventPayload ReleaseEventPayload Notes lease Lease \\| None Lease Acquisition payloads set lease=None when no proxy matched\u2014log these misses to spot exhausted pools. pool_name str str \\| None Release events carry the name stored on the lease. consumer_name str \u2014 Helps group metrics per worker/tenant. filters ProxyFilters \\| None \u2014 Reuse filters inside logs to debug misconfigured selectors. duration_ms int \u2014 How long the acquisition attempt took (including cleanup + storage calls). lease_duration_ms \u2014 int \\| None Milliseconds between acquired_at and released_at . pool_stats PoolStatsSnapshot \\| None PoolStatsSnapshot \\| None Snapshot collected after the operation: totals, available proxies, leases in-flight. All timestamps ( started_at , completed_at , released_at ) live in UTC so they can be compared safely across services. 2. Register Callbacks Once \u00b6 Attach callbacks right after instantiating the manager (for example during app startup or worker boot). Keep the functions fast\u2014push heavy lifting to async queues or background threads if needed. from pharox import AcquireEventPayload, ProxyManager, ReleaseEventPayload manager = ProxyManager(storage=storage) def on_acquire(event: AcquireEventPayload) -> None: ... def on_release(event: ReleaseEventPayload) -> None: ... manager.register_acquire_callback(on_acquire) manager.register_release_callback(on_release) Callbacks fire synchronously after each operation, so avoid blocking network calls inline unless you control their latency. 3. Emit Metrics \u00b6 Map the payload fields to timers and gauges in your telemetry stack. The example below uses a hypothetical StatsD client, but any metrics backend works: def on_acquire(event: AcquireEventPayload) -> None: tags = { \"pool\": event.pool_name, \"consumer\": event.consumer_name, \"status\": \"hit\" if event.lease else \"miss\", } metrics.timing(\"pharox.acquire.duration_ms\", event.duration_ms, tags) available = ( event.pool_stats.available_proxies if event.pool_stats else None ) if available is not None: metrics.gauge(\"pharox.pool.available\", available, tags) def on_release(event: ReleaseEventPayload) -> None: tags = {\"pool\": event.pool_name or \"unknown\"} metrics.timing( \"pharox.lease.duration_ms\", event.lease_duration_ms or 0, tags ) Recommended metrics: Acquisition latency (bucketed histogram) grouped by pool/consumer. Miss rate (lease is None ). Pool availability gauge ( available_proxies , active_proxies , total_leases ). Lease duration timers to detect stuck workloads. 4. Log Structured Events \u00b6 Structured logs make it easy to trace a proxy through your jobs. Include IDs, filters, and pool stats so you can replay what happened during an incident. def on_acquire(event: AcquireEventPayload) -> None: logger.info( \"pharox.acquire\", extra={ \"pool\": event.pool_name, \"consumer\": event.consumer_name, \"duration_ms\": event.duration_ms, \"filters\": event.filters.model_dump() if event.filters else None, \"lease_id\": event.lease.id if event.lease else None, \"available\": event.pool_stats.available_proxies if event.pool_stats else None, }, ) Log release events with the same correlation IDs ( lease.id , proxy_id ) so you can tie acquisitions to completions. 5. Handle Misses and Errors \u00b6 Acquisitions run callbacks even when no proxy is available. Use this to alert when a pool approaches exhaustion or a selector filter is too strict. Release callbacks may run after retries or failures. Use lease.released_at to compute custom SLAs or decide when to quarantine a proxy. If a callback raises, it bubbles up to the caller. Wrap fragile telemetry code in try/except blocks so that logging outages do not block proxy leasing. 6. Test Your Hooks \u00b6 In unit tests, register lightweight callbacks and capture the payloads in a list. Use the in-memory storage adapter so tests run without external services: def test_acquire_callback_records_miss(): storage = InMemoryStorage() manager = ProxyManager(storage=storage) events: list[AcquireEventPayload] = [] manager.register_acquire_callback(events.append) lease = manager.acquire_proxy(pool_name=\"nope\") assert lease is None assert len(events) == 1 payload = events[0] assert payload.lease is None assert payload.pool_name == \"nope\" Use similar assertions for release callbacks by seeding a proxy and calling manager.release_proxy . Next Steps \u00b6 Revisit the Proxy Manager deep dive for more context on acquisition internals. Embed the manager in a worker using the worker how-to once your hooks are in place. Wire the payload data into your alerting stack to catch saturation before it impacts users.","title":"Instrument Lifecycle Hooks"},{"location":"how-to/lifecycle-hooks/#instrument-lifecycle-hooks","text":"ProxyManager emits structured events every time it acquires or releases a lease. This guide shows how to connect those callbacks to your observability stack so you can track saturation, latency, and pool health without forking the toolkit.","title":"Instrument Lifecycle Hooks"},{"location":"how-to/lifecycle-hooks/#1-know-the-payloads","text":"Callbacks receive Pydantic models that are already validated and timezone-aware: Field AcquireEventPayload ReleaseEventPayload Notes lease Lease \\| None Lease Acquisition payloads set lease=None when no proxy matched\u2014log these misses to spot exhausted pools. pool_name str str \\| None Release events carry the name stored on the lease. consumer_name str \u2014 Helps group metrics per worker/tenant. filters ProxyFilters \\| None \u2014 Reuse filters inside logs to debug misconfigured selectors. duration_ms int \u2014 How long the acquisition attempt took (including cleanup + storage calls). lease_duration_ms \u2014 int \\| None Milliseconds between acquired_at and released_at . pool_stats PoolStatsSnapshot \\| None PoolStatsSnapshot \\| None Snapshot collected after the operation: totals, available proxies, leases in-flight. All timestamps ( started_at , completed_at , released_at ) live in UTC so they can be compared safely across services.","title":"1. Know the Payloads"},{"location":"how-to/lifecycle-hooks/#2-register-callbacks-once","text":"Attach callbacks right after instantiating the manager (for example during app startup or worker boot). Keep the functions fast\u2014push heavy lifting to async queues or background threads if needed. from pharox import AcquireEventPayload, ProxyManager, ReleaseEventPayload manager = ProxyManager(storage=storage) def on_acquire(event: AcquireEventPayload) -> None: ... def on_release(event: ReleaseEventPayload) -> None: ... manager.register_acquire_callback(on_acquire) manager.register_release_callback(on_release) Callbacks fire synchronously after each operation, so avoid blocking network calls inline unless you control their latency.","title":"2. Register Callbacks Once"},{"location":"how-to/lifecycle-hooks/#3-emit-metrics","text":"Map the payload fields to timers and gauges in your telemetry stack. The example below uses a hypothetical StatsD client, but any metrics backend works: def on_acquire(event: AcquireEventPayload) -> None: tags = { \"pool\": event.pool_name, \"consumer\": event.consumer_name, \"status\": \"hit\" if event.lease else \"miss\", } metrics.timing(\"pharox.acquire.duration_ms\", event.duration_ms, tags) available = ( event.pool_stats.available_proxies if event.pool_stats else None ) if available is not None: metrics.gauge(\"pharox.pool.available\", available, tags) def on_release(event: ReleaseEventPayload) -> None: tags = {\"pool\": event.pool_name or \"unknown\"} metrics.timing( \"pharox.lease.duration_ms\", event.lease_duration_ms or 0, tags ) Recommended metrics: Acquisition latency (bucketed histogram) grouped by pool/consumer. Miss rate (lease is None ). Pool availability gauge ( available_proxies , active_proxies , total_leases ). Lease duration timers to detect stuck workloads.","title":"3. Emit Metrics"},{"location":"how-to/lifecycle-hooks/#4-log-structured-events","text":"Structured logs make it easy to trace a proxy through your jobs. Include IDs, filters, and pool stats so you can replay what happened during an incident. def on_acquire(event: AcquireEventPayload) -> None: logger.info( \"pharox.acquire\", extra={ \"pool\": event.pool_name, \"consumer\": event.consumer_name, \"duration_ms\": event.duration_ms, \"filters\": event.filters.model_dump() if event.filters else None, \"lease_id\": event.lease.id if event.lease else None, \"available\": event.pool_stats.available_proxies if event.pool_stats else None, }, ) Log release events with the same correlation IDs ( lease.id , proxy_id ) so you can tie acquisitions to completions.","title":"4. Log Structured Events"},{"location":"how-to/lifecycle-hooks/#5-handle-misses-and-errors","text":"Acquisitions run callbacks even when no proxy is available. Use this to alert when a pool approaches exhaustion or a selector filter is too strict. Release callbacks may run after retries or failures. Use lease.released_at to compute custom SLAs or decide when to quarantine a proxy. If a callback raises, it bubbles up to the caller. Wrap fragile telemetry code in try/except blocks so that logging outages do not block proxy leasing.","title":"5. Handle Misses and Errors"},{"location":"how-to/lifecycle-hooks/#6-test-your-hooks","text":"In unit tests, register lightweight callbacks and capture the payloads in a list. Use the in-memory storage adapter so tests run without external services: def test_acquire_callback_records_miss(): storage = InMemoryStorage() manager = ProxyManager(storage=storage) events: list[AcquireEventPayload] = [] manager.register_acquire_callback(events.append) lease = manager.acquire_proxy(pool_name=\"nope\") assert lease is None assert len(events) == 1 payload = events[0] assert payload.lease is None assert payload.pool_name == \"nope\" Use similar assertions for release callbacks by seeding a proxy and calling manager.release_proxy .","title":"6. Test Your Hooks"},{"location":"how-to/lifecycle-hooks/#next-steps","text":"Revisit the Proxy Manager deep dive for more context on acquisition internals. Embed the manager in a worker using the worker how-to once your hooks are in place. Wire the payload data into your alerting stack to catch saturation before it impacts users.","title":"Next Steps"},{"location":"how-to/postgres-adapter/","text":"Build a PostgreSQL Adapter \u00b6 Pharox ships with an in-memory adapter for tests and demos. This guide shows how to create a PostgreSQL-backed implementation using SQLAlchemy. The same approach applies to other ORMs or async drivers\u2014swap libraries as needed. Scope This is a reference implementation you can adapt to your organisation's standards. It focuses on the required IStorage methods and leaves schema migrations to tools like Alembic. A maintained template (code + migrations + Docker Compose) lives under examples/postgres/ in the repository so you can clone it directly. Install optional dependencies The toolkit exposes a postgres extra that bundles SQLAlchemy, psycopg, and Alembic. Install it via pip install 'pharox[postgres]' (or poetry install --extras postgres ) before running the examples below. 1. Define the Schema \u00b6 Create tables for pools, proxies, consumers, and leases. Below is a simplified schema that captures the fields Pharox expects. CREATE TABLE proxy_pool ( id UUID PRIMARY KEY, name TEXT UNIQUE NOT NULL, description TEXT, created_at TIMESTAMPTZ NOT NULL DEFAULT NOW() ); CREATE TABLE consumer ( id UUID PRIMARY KEY, name TEXT UNIQUE NOT NULL, created_at TIMESTAMPTZ NOT NULL DEFAULT NOW() ); CREATE TABLE proxy ( id UUID PRIMARY KEY, pool_id UUID NOT NULL REFERENCES proxy_pool(id), host TEXT NOT NULL, port INTEGER NOT NULL, protocol TEXT NOT NULL, status TEXT NOT NULL, max_concurrency INTEGER NOT NULL DEFAULT 1, current_leases INTEGER NOT NULL DEFAULT 0, asn INTEGER, country TEXT, source TEXT, latitude DOUBLE PRECISION, longitude DOUBLE PRECISION, created_at TIMESTAMPTZ NOT NULL DEFAULT NOW() ); CREATE TABLE lease ( id UUID PRIMARY KEY, proxy_id UUID NOT NULL REFERENCES proxy(id), consumer_id UUID NOT NULL REFERENCES consumer(id), status TEXT NOT NULL, acquired_at TIMESTAMPTZ NOT NULL, expires_at TIMESTAMPTZ NOT NULL, released_at TIMESTAMPTZ ); Add indexes that match your filter workloads (e.g., proxy(pool_id, status) , geospatial indexes for latitude/longitude). 2. Implement the Adapter \u00b6 Use SQLAlchemy's ORM or Core\u2014below we use Core for clarity. from __future__ import annotations from datetime import UTC, datetime, timedelta from typing import Optional from uuid import uuid4 from sqlalchemy import select, update from sqlalchemy.dialects.postgresql import insert from sqlalchemy.engine import Connection from pharox.models import HealthCheckResult, Lease, Proxy, ProxyFilters from pharox.storage import IStorage from .tables import consumer_table, lease_table, pool_table, proxy_table class PostgresStorage(IStorage): def __init__(self, conn: Connection): self._conn = conn def find_available_proxy( self, pool_name: str, filters: Optional[ProxyFilters] = None ) -> Optional[Proxy]: query = ( select(proxy_table) .join(pool_table, proxy_table.c.pool_id == pool_table.c.id) .where( pool_table.c.name == pool_name, proxy_table.c.status == \"active\", proxy_table.c.current_leases < proxy_table.c.max_concurrency, ) .order_by(proxy_table.c.created_at.asc()) .limit(1) ) if filters: if filters.country: query = query.where(proxy_table.c.country == filters.country) if filters.source: query = query.where(proxy_table.c.source == filters.source) if filters.asn is not None: query = query.where(proxy_table.c.asn == filters.asn) row = self._conn.execute(query).m.fetchone() return Proxy.model_validate(row) if row else None def create_lease( self, proxy: Proxy, consumer_name: str, duration_seconds: int ) -> Lease: expires_at = datetime.now(UTC) + timedelta(seconds=duration_seconds) consumer_id = self.ensure_consumer(consumer_name) lease_id = uuid4() self._conn.execute( lease_table.insert().values( id=lease_id, proxy_id=proxy.id, consumer_id=consumer_id, status=\"active\", acquired_at=datetime.now(UTC), expires_at=expires_at, ) ) self._conn.execute( update(proxy_table) .where(proxy_table.c.id == proxy.id) .values(current_leases=proxy.current_leases + 1) ) return Lease( id=lease_id, proxy_id=proxy.id, consumer_id=consumer_id, status=\"active\", acquired_at=datetime.now(UTC), expires_at=expires_at, ) def ensure_consumer(self, consumer_name: str): stmt = ( insert(consumer_table) .values(id=uuid4(), name=consumer_name) .on_conflict_do_nothing(index_elements=[\"name\"]) .returning(consumer_table.c.id) ) result = self._conn.execute(stmt) row = result.fetchone() if row: return row.id query = select(consumer_table.c.id).where(consumer_table.c.name == consumer_name) return self._conn.execute(query).scalar_one() def release_lease(self, lease: Lease) -> None: self._conn.execute( update(lease_table) .where(lease_table.c.id == lease.id) .values(status=\"released\", released_at=datetime.now(UTC)) ) self._conn.execute( update(proxy_table) .where(proxy_table.c.id == lease.proxy_id) .values(current_leases=proxy_table.c.current_leases - 1) ) def cleanup_expired_leases(self) -> int: now = datetime.now(UTC) query = select(lease_table.c.id, lease_table.c.proxy_id).where( lease_table.c.status == \"active\", lease_table.c.expires_at <= now, ) expired = list(self._conn.execute(query)) for lease_id, proxy_id in expired: self._conn.execute( update(lease_table) .where(lease_table.c.id == lease_id) .values(status=\"expired\", released_at=now) ) self._conn.execute( update(proxy_table) .where(proxy_table.c.id == proxy_id) .values(current_leases=proxy_table.c.current_leases - 1) ) return len(expired) def apply_health_check_result( self, result: HealthCheckResult ) -> Optional[Proxy]: self._conn.execute( update(proxy_table) .where(proxy_table.c.id == result.proxy_id) .values( status=result.status.value, checked_at=result.checked_at, last_latency_ms=result.latency_ms, ) ) refreshed = self._conn.execute( select(proxy_table).where(proxy_table.c.id == result.proxy_id) ).m.fetchone() return Proxy.model_validate(refreshed) if refreshed else None The full implementation should guard against race conditions (e.g., using FOR UPDATE locks) and handle geospatial filters. Start simple, then iterate based on scale. Health result contract Align your apply_health_check_result with the guidance in Storage \u203a Best Practices so every adapter exposes consistent status/latency data to the toolkit. 3. Run Contract Tests \u00b6 Before adopting the adapter in production, reuse the storage contract suite bundled with Pharox: import pytest from sqlalchemy import create_engine, text from pharox.storage.postgres import PostgresStorage from pharox.models import Proxy, ProxyPool from pharox.tests.adapters import ( StorageContractFixtures, storage_contract_suite, ) engine = create_engine(\"postgresql+psycopg://user:pass@localhost/pharox\") def make_storage() -> PostgresStorage: with engine.begin() as conn: conn.execute(text(\"TRUNCATE lease, proxy, consumer, proxy_pool RESTART IDENTITY\")) return PostgresStorage(engine) def seed_pool(storage: PostgresStorage, pool: ProxyPool) -> ProxyPool: with engine.begin() as conn: conn.execute( text( \"INSERT INTO proxy_pool (id, name, description) VALUES (:id, :name, :description)\" ), {\"id\": str(pool.id), \"name\": pool.name, \"description\": pool.description}, ) return pool def seed_proxy(storage: PostgresStorage, proxy: Proxy) -> Proxy: with engine.begin() as conn: conn.execute( text( \"\"\" INSERT INTO proxy ( id, pool_id, host, port, protocol, status, max_concurrency, current_leases, country, source, city ) VALUES ( :id, :pool_id, :host, :port, :protocol, :status, :max_concurrency, 0, :country, :source, :city ) \"\"\" ), { \"id\": str(proxy.id), \"pool_id\": str(proxy.pool_id), \"host\": proxy.host, \"port\": proxy.port, \"protocol\": proxy.protocol.value, \"status\": proxy.status.value, \"max_concurrency\": proxy.max_concurrency, \"country\": proxy.country, \"source\": proxy.source, \"city\": proxy.city, }, ) return proxy @pytest.mark.contract def test_postgres_storage_contract(): fixtures = StorageContractFixtures( make_storage=make_storage, seed_pool=seed_pool, seed_proxy=seed_proxy, ) storage_contract_suite(fixtures) Set PHAROX_TEST_POSTGRES_URL to point at a disposable database (e.g., postgresql+psycopg://pharox:pharox@localhost:5439/pharox ) and run poetry run pytest tests/test_storage_contract_postgres.py in this repo to see the suite in action. Spin up PostgreSQL fast Use docker compose up postgres from the /examples/postgres directory (see below) to boot a development instance with migrations pre-applied. 4. Share as an Example \u00b6 Pharox already ships pharox.storage.postgres.PostgresStorage plus the examples/postgres/ toolkit (docker-compose, migrations, and shims). Copy that directory into your service to kick-start a production implementation, and send improvements upstream (docs, migrations, tests) so the template keeps getting better. If you need to publish an internal fork, keep the README up to date so new teams can bootstrap quickly.","title":"Build a PostgreSQL Adapter"},{"location":"how-to/postgres-adapter/#build-a-postgresql-adapter","text":"Pharox ships with an in-memory adapter for tests and demos. This guide shows how to create a PostgreSQL-backed implementation using SQLAlchemy. The same approach applies to other ORMs or async drivers\u2014swap libraries as needed. Scope This is a reference implementation you can adapt to your organisation's standards. It focuses on the required IStorage methods and leaves schema migrations to tools like Alembic. A maintained template (code + migrations + Docker Compose) lives under examples/postgres/ in the repository so you can clone it directly. Install optional dependencies The toolkit exposes a postgres extra that bundles SQLAlchemy, psycopg, and Alembic. Install it via pip install 'pharox[postgres]' (or poetry install --extras postgres ) before running the examples below.","title":"Build a PostgreSQL Adapter"},{"location":"how-to/postgres-adapter/#1-define-the-schema","text":"Create tables for pools, proxies, consumers, and leases. Below is a simplified schema that captures the fields Pharox expects. CREATE TABLE proxy_pool ( id UUID PRIMARY KEY, name TEXT UNIQUE NOT NULL, description TEXT, created_at TIMESTAMPTZ NOT NULL DEFAULT NOW() ); CREATE TABLE consumer ( id UUID PRIMARY KEY, name TEXT UNIQUE NOT NULL, created_at TIMESTAMPTZ NOT NULL DEFAULT NOW() ); CREATE TABLE proxy ( id UUID PRIMARY KEY, pool_id UUID NOT NULL REFERENCES proxy_pool(id), host TEXT NOT NULL, port INTEGER NOT NULL, protocol TEXT NOT NULL, status TEXT NOT NULL, max_concurrency INTEGER NOT NULL DEFAULT 1, current_leases INTEGER NOT NULL DEFAULT 0, asn INTEGER, country TEXT, source TEXT, latitude DOUBLE PRECISION, longitude DOUBLE PRECISION, created_at TIMESTAMPTZ NOT NULL DEFAULT NOW() ); CREATE TABLE lease ( id UUID PRIMARY KEY, proxy_id UUID NOT NULL REFERENCES proxy(id), consumer_id UUID NOT NULL REFERENCES consumer(id), status TEXT NOT NULL, acquired_at TIMESTAMPTZ NOT NULL, expires_at TIMESTAMPTZ NOT NULL, released_at TIMESTAMPTZ ); Add indexes that match your filter workloads (e.g., proxy(pool_id, status) , geospatial indexes for latitude/longitude).","title":"1. Define the Schema"},{"location":"how-to/postgres-adapter/#2-implement-the-adapter","text":"Use SQLAlchemy's ORM or Core\u2014below we use Core for clarity. from __future__ import annotations from datetime import UTC, datetime, timedelta from typing import Optional from uuid import uuid4 from sqlalchemy import select, update from sqlalchemy.dialects.postgresql import insert from sqlalchemy.engine import Connection from pharox.models import HealthCheckResult, Lease, Proxy, ProxyFilters from pharox.storage import IStorage from .tables import consumer_table, lease_table, pool_table, proxy_table class PostgresStorage(IStorage): def __init__(self, conn: Connection): self._conn = conn def find_available_proxy( self, pool_name: str, filters: Optional[ProxyFilters] = None ) -> Optional[Proxy]: query = ( select(proxy_table) .join(pool_table, proxy_table.c.pool_id == pool_table.c.id) .where( pool_table.c.name == pool_name, proxy_table.c.status == \"active\", proxy_table.c.current_leases < proxy_table.c.max_concurrency, ) .order_by(proxy_table.c.created_at.asc()) .limit(1) ) if filters: if filters.country: query = query.where(proxy_table.c.country == filters.country) if filters.source: query = query.where(proxy_table.c.source == filters.source) if filters.asn is not None: query = query.where(proxy_table.c.asn == filters.asn) row = self._conn.execute(query).m.fetchone() return Proxy.model_validate(row) if row else None def create_lease( self, proxy: Proxy, consumer_name: str, duration_seconds: int ) -> Lease: expires_at = datetime.now(UTC) + timedelta(seconds=duration_seconds) consumer_id = self.ensure_consumer(consumer_name) lease_id = uuid4() self._conn.execute( lease_table.insert().values( id=lease_id, proxy_id=proxy.id, consumer_id=consumer_id, status=\"active\", acquired_at=datetime.now(UTC), expires_at=expires_at, ) ) self._conn.execute( update(proxy_table) .where(proxy_table.c.id == proxy.id) .values(current_leases=proxy.current_leases + 1) ) return Lease( id=lease_id, proxy_id=proxy.id, consumer_id=consumer_id, status=\"active\", acquired_at=datetime.now(UTC), expires_at=expires_at, ) def ensure_consumer(self, consumer_name: str): stmt = ( insert(consumer_table) .values(id=uuid4(), name=consumer_name) .on_conflict_do_nothing(index_elements=[\"name\"]) .returning(consumer_table.c.id) ) result = self._conn.execute(stmt) row = result.fetchone() if row: return row.id query = select(consumer_table.c.id).where(consumer_table.c.name == consumer_name) return self._conn.execute(query).scalar_one() def release_lease(self, lease: Lease) -> None: self._conn.execute( update(lease_table) .where(lease_table.c.id == lease.id) .values(status=\"released\", released_at=datetime.now(UTC)) ) self._conn.execute( update(proxy_table) .where(proxy_table.c.id == lease.proxy_id) .values(current_leases=proxy_table.c.current_leases - 1) ) def cleanup_expired_leases(self) -> int: now = datetime.now(UTC) query = select(lease_table.c.id, lease_table.c.proxy_id).where( lease_table.c.status == \"active\", lease_table.c.expires_at <= now, ) expired = list(self._conn.execute(query)) for lease_id, proxy_id in expired: self._conn.execute( update(lease_table) .where(lease_table.c.id == lease_id) .values(status=\"expired\", released_at=now) ) self._conn.execute( update(proxy_table) .where(proxy_table.c.id == proxy_id) .values(current_leases=proxy_table.c.current_leases - 1) ) return len(expired) def apply_health_check_result( self, result: HealthCheckResult ) -> Optional[Proxy]: self._conn.execute( update(proxy_table) .where(proxy_table.c.id == result.proxy_id) .values( status=result.status.value, checked_at=result.checked_at, last_latency_ms=result.latency_ms, ) ) refreshed = self._conn.execute( select(proxy_table).where(proxy_table.c.id == result.proxy_id) ).m.fetchone() return Proxy.model_validate(refreshed) if refreshed else None The full implementation should guard against race conditions (e.g., using FOR UPDATE locks) and handle geospatial filters. Start simple, then iterate based on scale. Health result contract Align your apply_health_check_result with the guidance in Storage \u203a Best Practices so every adapter exposes consistent status/latency data to the toolkit.","title":"2. Implement the Adapter"},{"location":"how-to/postgres-adapter/#3-run-contract-tests","text":"Before adopting the adapter in production, reuse the storage contract suite bundled with Pharox: import pytest from sqlalchemy import create_engine, text from pharox.storage.postgres import PostgresStorage from pharox.models import Proxy, ProxyPool from pharox.tests.adapters import ( StorageContractFixtures, storage_contract_suite, ) engine = create_engine(\"postgresql+psycopg://user:pass@localhost/pharox\") def make_storage() -> PostgresStorage: with engine.begin() as conn: conn.execute(text(\"TRUNCATE lease, proxy, consumer, proxy_pool RESTART IDENTITY\")) return PostgresStorage(engine) def seed_pool(storage: PostgresStorage, pool: ProxyPool) -> ProxyPool: with engine.begin() as conn: conn.execute( text( \"INSERT INTO proxy_pool (id, name, description) VALUES (:id, :name, :description)\" ), {\"id\": str(pool.id), \"name\": pool.name, \"description\": pool.description}, ) return pool def seed_proxy(storage: PostgresStorage, proxy: Proxy) -> Proxy: with engine.begin() as conn: conn.execute( text( \"\"\" INSERT INTO proxy ( id, pool_id, host, port, protocol, status, max_concurrency, current_leases, country, source, city ) VALUES ( :id, :pool_id, :host, :port, :protocol, :status, :max_concurrency, 0, :country, :source, :city ) \"\"\" ), { \"id\": str(proxy.id), \"pool_id\": str(proxy.pool_id), \"host\": proxy.host, \"port\": proxy.port, \"protocol\": proxy.protocol.value, \"status\": proxy.status.value, \"max_concurrency\": proxy.max_concurrency, \"country\": proxy.country, \"source\": proxy.source, \"city\": proxy.city, }, ) return proxy @pytest.mark.contract def test_postgres_storage_contract(): fixtures = StorageContractFixtures( make_storage=make_storage, seed_pool=seed_pool, seed_proxy=seed_proxy, ) storage_contract_suite(fixtures) Set PHAROX_TEST_POSTGRES_URL to point at a disposable database (e.g., postgresql+psycopg://pharox:pharox@localhost:5439/pharox ) and run poetry run pytest tests/test_storage_contract_postgres.py in this repo to see the suite in action. Spin up PostgreSQL fast Use docker compose up postgres from the /examples/postgres directory (see below) to boot a development instance with migrations pre-applied.","title":"3. Run Contract Tests"},{"location":"how-to/postgres-adapter/#4-share-as-an-example","text":"Pharox already ships pharox.storage.postgres.PostgresStorage plus the examples/postgres/ toolkit (docker-compose, migrations, and shims). Copy that directory into your service to kick-start a production implementation, and send improvements upstream (docs, migrations, tests) so the template keeps getting better. If you need to publish an internal fork, keep the README up to date so new teams can bootstrap quickly.","title":"4. Share as an Example"},{"location":"reference/bootstrap/","text":"Bootstrap Helpers \u00b6 The pharox.utils.bootstrap module provides convenience functions that seed storage adapters with consumers, pools, and proxies. They are optional but help reduce boilerplate in tests, documentation, and quick demos. from pharox import bootstrap_consumer, bootstrap_pool, bootstrap_proxy bootstrap_consumer \u00b6 def bootstrap_consumer( storage: IStorage, *, name: str = \"default-consumer\", consumer_id: UUID | None = None, ) -> Consumer Adds a Consumer to storage using storage.add_consumer when available. Falls back to storage.ensure_consumer , returning a Consumer with the UUID from the adapter. Useful for seeding named tenants during tests. bootstrap_pool \u00b6 def bootstrap_pool( storage: IStorage, *, name: str = \"default-pool\", description: str | None = None, pool_id: UUID | None = None, ) -> ProxyPool Requires the storage adapter to expose add_pool . Returns the stored ProxyPool . Raise an AttributeError when the adapter cannot add pools directly\u2014ideal for catching unsupported helpers in production adapters. bootstrap_proxy \u00b6 def bootstrap_proxy( storage: IStorage, *, pool: ProxyPool, host: str, port: int, protocol: ProxyProtocol = ProxyProtocol.HTTP, status: ProxyStatus = ProxyStatus.ACTIVE, proxy_id: UUID | None = None, **extra_fields: Any, ) -> Proxy Expects storage.add_proxy to be available. Returns the stored Proxy ; if the adapter implements get_proxy_by_id , the helper fetches the persisted copy to capture computed fields (e.g., defaults). Accepts additional keyword arguments to populate provider metadata, auth credentials, or geospatial data. When to Use Them \u00b6 Context Recommendation Unit tests Seed fixtures quickly without writing adapter-specific code. Tutorials & notebooks Keep focus on orchestration logic, not boilerplate. Production bootstrap Implement environment-specific scripts instead of using these helpers directly. Related Topics \u00b6 Quickstart Embed Pharox in a Worker ProxyManager reference","title":"Bootstrap Helpers"},{"location":"reference/bootstrap/#bootstrap-helpers","text":"The pharox.utils.bootstrap module provides convenience functions that seed storage adapters with consumers, pools, and proxies. They are optional but help reduce boilerplate in tests, documentation, and quick demos. from pharox import bootstrap_consumer, bootstrap_pool, bootstrap_proxy","title":"Bootstrap Helpers"},{"location":"reference/bootstrap/#bootstrap_consumer","text":"def bootstrap_consumer( storage: IStorage, *, name: str = \"default-consumer\", consumer_id: UUID | None = None, ) -> Consumer Adds a Consumer to storage using storage.add_consumer when available. Falls back to storage.ensure_consumer , returning a Consumer with the UUID from the adapter. Useful for seeding named tenants during tests.","title":"bootstrap_consumer"},{"location":"reference/bootstrap/#bootstrap_pool","text":"def bootstrap_pool( storage: IStorage, *, name: str = \"default-pool\", description: str | None = None, pool_id: UUID | None = None, ) -> ProxyPool Requires the storage adapter to expose add_pool . Returns the stored ProxyPool . Raise an AttributeError when the adapter cannot add pools directly\u2014ideal for catching unsupported helpers in production adapters.","title":"bootstrap_pool"},{"location":"reference/bootstrap/#bootstrap_proxy","text":"def bootstrap_proxy( storage: IStorage, *, pool: ProxyPool, host: str, port: int, protocol: ProxyProtocol = ProxyProtocol.HTTP, status: ProxyStatus = ProxyStatus.ACTIVE, proxy_id: UUID | None = None, **extra_fields: Any, ) -> Proxy Expects storage.add_proxy to be available. Returns the stored Proxy ; if the adapter implements get_proxy_by_id , the helper fetches the persisted copy to capture computed fields (e.g., defaults). Accepts additional keyword arguments to populate provider metadata, auth credentials, or geospatial data.","title":"bootstrap_proxy"},{"location":"reference/bootstrap/#when-to-use-them","text":"Context Recommendation Unit tests Seed fixtures quickly without writing adapter-specific code. Tutorials & notebooks Keep focus on orchestration logic, not boilerplate. Production bootstrap Implement environment-specific scripts instead of using these helpers directly.","title":"When to Use Them"},{"location":"reference/bootstrap/#related-topics","text":"Quickstart Embed Pharox in a Worker ProxyManager reference","title":"Related Topics"},{"location":"reference/proxy-manager/","text":"ProxyManager \u00b6 ProxyManager orchestrates leasing, releasing, and maintenance of proxies while delegating persistence to an IStorage adapter. from pharox import ProxyManager manager = ProxyManager(storage) Constructor \u00b6 ProxyManager(storage: IStorage) storage : Concrete implementation of pharox.storage.IStorage . The manager keeps no in-memory state besides registered callbacks. Core Methods \u00b6 acquire_proxy \u00b6 def acquire_proxy( pool_name: str, consumer_name: str | None = None, duration_seconds: int = 300, filters: ProxyFilters | None = None, ) -> Lease | None Validates duration_seconds > 0 . Auto-registers the default consumer ( \"default\" ) using storage.ensure_consumer . Calls storage.cleanup_expired_leases() before searching for a proxy. Uses storage.find_available_proxy + storage.create_lease . Returns None when no eligible proxy exists. release_proxy \u00b6 def release_proxy(lease: Lease) -> None Delegates to storage.release_lease . Triggers release callbacks after storage completes. cleanup_expired_leases \u00b6 def cleanup_expired_leases() -> int Pass-through to storage.cleanup_expired_leases . Returns number of leases released. with_lease \u00b6 @contextmanager def with_lease( pool_name: str, consumer_name: str | None = None, duration_seconds: int = 300, filters: ProxyFilters | None = None, ) -> Iterator[Lease | None] Wraps acquire_proxy and guarantees release_proxy in a finally block. Yields None when acquisition fails so callers can retry gracefully. Callback Registration \u00b6 from pharox import ( AcquireEventPayload, ReleaseEventPayload, ) manager.register_acquire_callback(Callable[[AcquireEventPayload], None]) manager.register_release_callback(Callable[[ReleaseEventPayload], None]) AcquireEventPayload includes the resulting Lease (or None ), the pool and consumer names, resolved filters, started_at / completed_at timestamps, the execution duration in milliseconds, and a PoolStatsSnapshot . ReleaseEventPayload contains the released Lease , released_at , computed lease duration, and the same pool stats snapshot captured after the release. Callbacks run synchronously; keep them lightweight or hand off to background workers. PoolStatsSnapshot \u00b6 PoolStatsSnapshot reports aggregated counts per pool: total_proxies , active_proxies , available_proxies leased_proxies (proxies with at least one active lease) total_leases (sum of current_leases across the pool) collected_at timestamp for when the snapshot was generated Async Helpers \u00b6 The core manager is synchronous, but Pharox exposes thin wrappers to make it ergonomic in asyncio applications: from pharox import ( acquire_proxy_async, release_proxy_async, with_lease_async, ) acquire_proxy_async and release_proxy_async delegate to the synchronous manager via asyncio.to_thread , keeping event loops responsive. with_lease_async mirrors the synchronous context manager while ensuring leases are released when the async block exits\u2014even if an exception occurs. Use these helpers whenever your storage adapter is synchronous but the calling code runs inside an async worker or FastAPI route handler. Default Consumer Name \u00b6 ProxyManager.DEFAULT_CONSUMER_NAME == \"default\" . Auto-created on first acquisition without an explicit consumer_name . Best Practices \u00b6 Register callbacks once at process startup to avoid duplicate telemetry. Combine with_lease with try/except if you need custom error handling. Schedule cleanup_expired_leases periodically for long-running services. Use ProxyFilters to offload selection logic to storage adapters.","title":"ProxyManager API"},{"location":"reference/proxy-manager/#proxymanager","text":"ProxyManager orchestrates leasing, releasing, and maintenance of proxies while delegating persistence to an IStorage adapter. from pharox import ProxyManager manager = ProxyManager(storage)","title":"ProxyManager"},{"location":"reference/proxy-manager/#constructor","text":"ProxyManager(storage: IStorage) storage : Concrete implementation of pharox.storage.IStorage . The manager keeps no in-memory state besides registered callbacks.","title":"Constructor"},{"location":"reference/proxy-manager/#core-methods","text":"","title":"Core Methods"},{"location":"reference/proxy-manager/#acquire_proxy","text":"def acquire_proxy( pool_name: str, consumer_name: str | None = None, duration_seconds: int = 300, filters: ProxyFilters | None = None, ) -> Lease | None Validates duration_seconds > 0 . Auto-registers the default consumer ( \"default\" ) using storage.ensure_consumer . Calls storage.cleanup_expired_leases() before searching for a proxy. Uses storage.find_available_proxy + storage.create_lease . Returns None when no eligible proxy exists.","title":"acquire_proxy"},{"location":"reference/proxy-manager/#release_proxy","text":"def release_proxy(lease: Lease) -> None Delegates to storage.release_lease . Triggers release callbacks after storage completes.","title":"release_proxy"},{"location":"reference/proxy-manager/#cleanup_expired_leases","text":"def cleanup_expired_leases() -> int Pass-through to storage.cleanup_expired_leases . Returns number of leases released.","title":"cleanup_expired_leases"},{"location":"reference/proxy-manager/#with_lease","text":"@contextmanager def with_lease( pool_name: str, consumer_name: str | None = None, duration_seconds: int = 300, filters: ProxyFilters | None = None, ) -> Iterator[Lease | None] Wraps acquire_proxy and guarantees release_proxy in a finally block. Yields None when acquisition fails so callers can retry gracefully.","title":"with_lease"},{"location":"reference/proxy-manager/#callback-registration","text":"from pharox import ( AcquireEventPayload, ReleaseEventPayload, ) manager.register_acquire_callback(Callable[[AcquireEventPayload], None]) manager.register_release_callback(Callable[[ReleaseEventPayload], None]) AcquireEventPayload includes the resulting Lease (or None ), the pool and consumer names, resolved filters, started_at / completed_at timestamps, the execution duration in milliseconds, and a PoolStatsSnapshot . ReleaseEventPayload contains the released Lease , released_at , computed lease duration, and the same pool stats snapshot captured after the release. Callbacks run synchronously; keep them lightweight or hand off to background workers.","title":"Callback Registration"},{"location":"reference/proxy-manager/#poolstatssnapshot","text":"PoolStatsSnapshot reports aggregated counts per pool: total_proxies , active_proxies , available_proxies leased_proxies (proxies with at least one active lease) total_leases (sum of current_leases across the pool) collected_at timestamp for when the snapshot was generated","title":"PoolStatsSnapshot"},{"location":"reference/proxy-manager/#async-helpers","text":"The core manager is synchronous, but Pharox exposes thin wrappers to make it ergonomic in asyncio applications: from pharox import ( acquire_proxy_async, release_proxy_async, with_lease_async, ) acquire_proxy_async and release_proxy_async delegate to the synchronous manager via asyncio.to_thread , keeping event loops responsive. with_lease_async mirrors the synchronous context manager while ensuring leases are released when the async block exits\u2014even if an exception occurs. Use these helpers whenever your storage adapter is synchronous but the calling code runs inside an async worker or FastAPI route handler.","title":"Async Helpers"},{"location":"reference/proxy-manager/#default-consumer-name","text":"ProxyManager.DEFAULT_CONSUMER_NAME == \"default\" . Auto-created on first acquisition without an explicit consumer_name .","title":"Default Consumer Name"},{"location":"reference/proxy-manager/#best-practices","text":"Register callbacks once at process startup to avoid duplicate telemetry. Combine with_lease with try/except if you need custom error handling. Schedule cleanup_expired_leases periodically for long-running services. Use ProxyFilters to offload selection logic to storage adapters.","title":"Best Practices"}]}