{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Lighthouse Toolkit \u00b6 Welcome to the documentation hub for the Lighthouse proxy-management toolkit. This site focuses on the components you embed inside scripts, workers, or services. What You Get \u00b6 Proxy lifecycle management: Acquire and release proxies with concurrency limits and lease tracking handled for you. Flexible storage adapters: Implement the IStorage interface to back the toolkit with any persistence layer (PostgreSQL, Redis, in-memory, etc.). Health checking helpers: Run protocol-aware connectivity probes that the SDK and FastAPI service can reuse. Type-safe models: Pydantic v2 models define the schema shared across every Lighthouse component. Quickstart \u00b6 Install the package from PyPI and wire the in-memory storage to experiment locally: pip install lighthouse from lighthouse import ( Consumer, InMemoryStorage, Proxy, ProxyManager, ProxyPool, ProxyStatus, ) storage = InMemoryStorage() manager = ProxyManager(storage=storage) # Seed a default consumer and pool consumer = Consumer(name=manager.DEFAULT_CONSUMER_NAME) storage.add_consumer(consumer) pool = ProxyPool(name=\"latam-residential\") storage.add_pool(pool) storage.add_proxy( Proxy( host=\"1.1.1.1\", port=8080, protocol=\"http\", pool_id=pool.id, status=ProxyStatus.ACTIVE, ) ) lease = manager.acquire_proxy(pool_name=pool.name) if lease: print(\"Proxy leased!\", lease.proxy_id) manager.release_proxy(lease) For more advanced examples and real-host testing, explore the draft script in drafts/run_proxy_health_checks.py . Where to Go Next \u00b6 Understand leasing flows in the Proxy Manager guide . Review available models and filters in Models & Filters . Explore the configurable Health Checks . Learn how to plug your database in Storage Adapters . Visit the GitHub repository for issue tracking and release notes: https://github.com/fzaca/lighthouse .","title":"Home"},{"location":"#lighthouse-toolkit","text":"Welcome to the documentation hub for the Lighthouse proxy-management toolkit. This site focuses on the components you embed inside scripts, workers, or services.","title":"Lighthouse Toolkit"},{"location":"#what-you-get","text":"Proxy lifecycle management: Acquire and release proxies with concurrency limits and lease tracking handled for you. Flexible storage adapters: Implement the IStorage interface to back the toolkit with any persistence layer (PostgreSQL, Redis, in-memory, etc.). Health checking helpers: Run protocol-aware connectivity probes that the SDK and FastAPI service can reuse. Type-safe models: Pydantic v2 models define the schema shared across every Lighthouse component.","title":"What You Get"},{"location":"#quickstart","text":"Install the package from PyPI and wire the in-memory storage to experiment locally: pip install lighthouse from lighthouse import ( Consumer, InMemoryStorage, Proxy, ProxyManager, ProxyPool, ProxyStatus, ) storage = InMemoryStorage() manager = ProxyManager(storage=storage) # Seed a default consumer and pool consumer = Consumer(name=manager.DEFAULT_CONSUMER_NAME) storage.add_consumer(consumer) pool = ProxyPool(name=\"latam-residential\") storage.add_pool(pool) storage.add_proxy( Proxy( host=\"1.1.1.1\", port=8080, protocol=\"http\", pool_id=pool.id, status=ProxyStatus.ACTIVE, ) ) lease = manager.acquire_proxy(pool_name=pool.name) if lease: print(\"Proxy leased!\", lease.proxy_id) manager.release_proxy(lease) For more advanced examples and real-host testing, explore the draft script in drafts/run_proxy_health_checks.py .","title":"Quickstart"},{"location":"#where-to-go-next","text":"Understand leasing flows in the Proxy Manager guide . Review available models and filters in Models & Filters . Explore the configurable Health Checks . Learn how to plug your database in Storage Adapters . Visit the GitHub repository for issue tracking and release notes: https://github.com/fzaca/lighthouse .","title":"Where to Go Next"},{"location":"health-checks/","text":"Health Checking Toolkit \u00b6 Every Lighthouse deployment needs a consistent, repeatable way to verify that proxies are reachable and responsive. The health module bundles that logic so scripts, SDK workers, and services all classify proxies the same way. Key Building Blocks \u00b6 HealthCheckOptions ( lighthouse.models ): runtime configuration for a probe (target URL, attempts, timeout, expected status codes, latency threshold, request headers, redirect policy). HealthChecker ( lighthouse.health ): orchestrates checks, choosing the strategy that matches each proxy\u2019s protocol and returning a HealthCheckResult . HTTPHealthCheckStrategy : default strategy used for HTTP, HTTPS, SOCKS4, and SOCKS5 proxies. It performs real HTTP requests through the proxy and classifies the result as active , slow , or inactive . You can register additional strategies for custom protocols or handshake behaviour, while reusing the rest of the orchestration logic. Quick Example \u00b6 import asyncio from uuid import uuid4 from lighthouse import HealthCheckOptions, HealthChecker, Proxy, ProxyProtocol proxy = Proxy( host=\"dc.oxylabs.io\", port=8001, protocol=ProxyProtocol.HTTP, pool_id=uuid4(), # replace with an existing pool ID in your storage ) checker = HealthChecker() options = HealthCheckOptions( target_url=\"https://example.com/status/204\", expected_status_codes=[204], attempts=2, timeout=5.0, ) result = asyncio.run(checker.check_proxy(proxy, options=options)) print(result.status, result.latency_ms, result.status_code) Options Reference \u00b6 Field Purpose Tips target_url Endpoint hit through the proxy Use a low-latency endpoint under your control when possible. timeout Per-attempt timeout (seconds) Keep conservative defaults; health checks should fail fast. attempts Maximum retries Combine with expected_status_codes to tolerate transient errors. expected_status_codes Acceptable HTTP codes Include every success code returned by your target URL. slow_threshold_ms Latency boundary between active and slow Tune per workload. slow still indicates connectivity. headers Extra request headers Useful for provider auth tokens or tracing headers. allow_redirects Follow HTTP redirects Disable when you want to detect 3xx responses explicitly. Batch Checks with Streaming \u00b6 HealthChecker.stream_health_checks launches checks concurrently and yields results as they complete: from lighthouse import ProxyStatus async def sweep(proxies): checker = HealthChecker() async for result in checker.stream_health_checks(proxies): if result.status in {ProxyStatus.ACTIVE, ProxyStatus.SLOW}: handle_healthy_proxy(result) else: handle_unhealthy_proxy(result) asyncio.run(sweep(proxy_list)) This pattern is ideal for scheduled health workers or on-demand diagnostics. The async generator keeps memory usage predictable even when you probe large lists. Integrating with ProxyManager \u00b6 Health checks often precede or follow leasing operations: Acquire a candidate proxy using ProxyManager.acquire_proxy . Run a health check with the desired options. Release the lease if the proxy fails or exhibits high latency. from lighthouse import ProxyStatus async def acquire_and_probe(manager, storage, checker): lease = manager.acquire_proxy(pool_name=\"latam-residential\") if not lease: return None proxy = storage.get_proxy_by_id(lease.proxy_id) health = await checker.check_proxy(proxy) if health.status is ProxyStatus.INACTIVE: manager.release_proxy(lease) return None return lease The workflow mirrors the behaviour you\u2019ll deploy in the FastAPI service or SDK workers, ensuring both components make consistent decisions. Custom Strategies \u00b6 Some environments require protocol-specific probes (for example, negotiating a SOCKS5 authentication step or verifying a proprietary tunnel). Implement HealthCheckStrategy and register it for the relevant protocol: from lighthouse import ProxyProtocol from lighthouse.health import HealthCheckStrategy, HealthChecker class RedisTunnelStrategy(HealthCheckStrategy): async def check(self, proxy, options): ... # perform tunnel handshake and return HealthCheckResult checker = HealthChecker() checker.register_strategy(ProxyProtocol.SOCKS5, RedisTunnelStrategy()) Storing and Acting on Results \u00b6 HealthCheckResult captures: status ( ProxyStatus.ACTIVE , SLOW , or INACTIVE ) latency_ms (per successful attempt or timeout duration) attempts used before reaching a decision status_code and optional error_message checked_at timestamp (UTC) Persist these results in your own layer if you need historical analytics or automated remediation. The toolkit intentionally stops short of storing anything so it stays embeddable in any environment. Local Testing Helpers \u00b6 The repository includes drafts/run_proxy_health_checks.py , a small script that seeds InMemoryStorage , leases proxies, and runs simple or bulk health checks. Use it as a reference when wiring the toolkit to real providers or when debugging proxy credentials.","title":"Health Checks"},{"location":"health-checks/#health-checking-toolkit","text":"Every Lighthouse deployment needs a consistent, repeatable way to verify that proxies are reachable and responsive. The health module bundles that logic so scripts, SDK workers, and services all classify proxies the same way.","title":"Health Checking Toolkit"},{"location":"health-checks/#key-building-blocks","text":"HealthCheckOptions ( lighthouse.models ): runtime configuration for a probe (target URL, attempts, timeout, expected status codes, latency threshold, request headers, redirect policy). HealthChecker ( lighthouse.health ): orchestrates checks, choosing the strategy that matches each proxy\u2019s protocol and returning a HealthCheckResult . HTTPHealthCheckStrategy : default strategy used for HTTP, HTTPS, SOCKS4, and SOCKS5 proxies. It performs real HTTP requests through the proxy and classifies the result as active , slow , or inactive . You can register additional strategies for custom protocols or handshake behaviour, while reusing the rest of the orchestration logic.","title":"Key Building Blocks"},{"location":"health-checks/#quick-example","text":"import asyncio from uuid import uuid4 from lighthouse import HealthCheckOptions, HealthChecker, Proxy, ProxyProtocol proxy = Proxy( host=\"dc.oxylabs.io\", port=8001, protocol=ProxyProtocol.HTTP, pool_id=uuid4(), # replace with an existing pool ID in your storage ) checker = HealthChecker() options = HealthCheckOptions( target_url=\"https://example.com/status/204\", expected_status_codes=[204], attempts=2, timeout=5.0, ) result = asyncio.run(checker.check_proxy(proxy, options=options)) print(result.status, result.latency_ms, result.status_code)","title":"Quick Example"},{"location":"health-checks/#options-reference","text":"Field Purpose Tips target_url Endpoint hit through the proxy Use a low-latency endpoint under your control when possible. timeout Per-attempt timeout (seconds) Keep conservative defaults; health checks should fail fast. attempts Maximum retries Combine with expected_status_codes to tolerate transient errors. expected_status_codes Acceptable HTTP codes Include every success code returned by your target URL. slow_threshold_ms Latency boundary between active and slow Tune per workload. slow still indicates connectivity. headers Extra request headers Useful for provider auth tokens or tracing headers. allow_redirects Follow HTTP redirects Disable when you want to detect 3xx responses explicitly.","title":"Options Reference"},{"location":"health-checks/#batch-checks-with-streaming","text":"HealthChecker.stream_health_checks launches checks concurrently and yields results as they complete: from lighthouse import ProxyStatus async def sweep(proxies): checker = HealthChecker() async for result in checker.stream_health_checks(proxies): if result.status in {ProxyStatus.ACTIVE, ProxyStatus.SLOW}: handle_healthy_proxy(result) else: handle_unhealthy_proxy(result) asyncio.run(sweep(proxy_list)) This pattern is ideal for scheduled health workers or on-demand diagnostics. The async generator keeps memory usage predictable even when you probe large lists.","title":"Batch Checks with Streaming"},{"location":"health-checks/#integrating-with-proxymanager","text":"Health checks often precede or follow leasing operations: Acquire a candidate proxy using ProxyManager.acquire_proxy . Run a health check with the desired options. Release the lease if the proxy fails or exhibits high latency. from lighthouse import ProxyStatus async def acquire_and_probe(manager, storage, checker): lease = manager.acquire_proxy(pool_name=\"latam-residential\") if not lease: return None proxy = storage.get_proxy_by_id(lease.proxy_id) health = await checker.check_proxy(proxy) if health.status is ProxyStatus.INACTIVE: manager.release_proxy(lease) return None return lease The workflow mirrors the behaviour you\u2019ll deploy in the FastAPI service or SDK workers, ensuring both components make consistent decisions.","title":"Integrating with ProxyManager"},{"location":"health-checks/#custom-strategies","text":"Some environments require protocol-specific probes (for example, negotiating a SOCKS5 authentication step or verifying a proprietary tunnel). Implement HealthCheckStrategy and register it for the relevant protocol: from lighthouse import ProxyProtocol from lighthouse.health import HealthCheckStrategy, HealthChecker class RedisTunnelStrategy(HealthCheckStrategy): async def check(self, proxy, options): ... # perform tunnel handshake and return HealthCheckResult checker = HealthChecker() checker.register_strategy(ProxyProtocol.SOCKS5, RedisTunnelStrategy())","title":"Custom Strategies"},{"location":"health-checks/#storing-and-acting-on-results","text":"HealthCheckResult captures: status ( ProxyStatus.ACTIVE , SLOW , or INACTIVE ) latency_ms (per successful attempt or timeout duration) attempts used before reaching a decision status_code and optional error_message checked_at timestamp (UTC) Persist these results in your own layer if you need historical analytics or automated remediation. The toolkit intentionally stops short of storing anything so it stays embeddable in any environment.","title":"Storing and Acting on Results"},{"location":"health-checks/#local-testing-helpers","text":"The repository includes drafts/run_proxy_health_checks.py , a small script that seeds InMemoryStorage , leases proxies, and runs simple or bulk health checks. Use it as a reference when wiring the toolkit to real providers or when debugging proxy credentials.","title":"Local Testing Helpers"},{"location":"models/","text":"Models & Filters \u00b6 The toolkit ships Pydantic models that describe proxies, leases, and filtering options. These classes are shared across the SDK and service, so treating them as your contract keeps every integration aligned. Proxy \u00b6 from lighthouse import Proxy, ProxyProtocol, ProxyStatus proxy = Proxy( host=\"186.33.123.10\", port=8080, protocol=ProxyProtocol.HTTP, pool_id=pool.id, status=ProxyStatus.ACTIVE, country=\"AR\", source=\"oxylabs\", ) print(proxy.url) # http://186.33.123.10:8080 Important fields: protocol accepts HTTP, HTTPS, SOCKS4, or SOCKS5. credentials can hold user/password pairs. The url property encodes them automatically for client libraries like httpx or requests . max_concurrency limits simultaneous leases. None means unlimited. current_leases is managed by the storage adapter; treat it as read-only. ProxyPool \u00b6 Pools group proxies with shared configuration. Use pool names to pick distinct provider buckets or geographic segments. from lighthouse import ProxyPool pool = ProxyPool(name=\"latam-residential\", description=\"Residential LatAm proxies\") Consumers \u00b6 A Consumer identifies the actor leasing proxies. This can be a worker name, a service, or any identifier meaningful to your system. from lighthouse import Consumer storage.add_consumer(Consumer(name=\"worker-1\")) Leases \u00b6 Lease records who is using a proxy and when the access expires. You usually interact with this through ProxyManager.acquire_proxy , but the data model is available if you need to persist or audit leases. lease.id lease.proxy_id lease.expires_at Proxy Filters \u00b6 ProxyFilters lets you express selection criteria. All fields are optional, and adapters decide how to evaluate them. from lighthouse import ProxyFilters filters = ProxyFilters( country=\"CO\", source=\"latam-provider\", asn=12345, latitude=4.7110, longitude=-74.0721, radius_km=50, ) Validation rules: Latitude and longitude must appear together. If radius_km is set, geolocation coordinates are required. Status Enums \u00b6 Both proxies and leases expose enums for their lifecycle. from lighthouse import ProxyStatus, LeaseStatus if result.status is ProxyStatus.SLOW: ... if lease.status is LeaseStatus.RELEASED: ... Stay consistent with these enums when writing storage adapters or APIs so that all Lighthouse components can reason about state transitions identically.","title":"Models & Filters"},{"location":"models/#models-filters","text":"The toolkit ships Pydantic models that describe proxies, leases, and filtering options. These classes are shared across the SDK and service, so treating them as your contract keeps every integration aligned.","title":"Models &amp; Filters"},{"location":"models/#proxy","text":"from lighthouse import Proxy, ProxyProtocol, ProxyStatus proxy = Proxy( host=\"186.33.123.10\", port=8080, protocol=ProxyProtocol.HTTP, pool_id=pool.id, status=ProxyStatus.ACTIVE, country=\"AR\", source=\"oxylabs\", ) print(proxy.url) # http://186.33.123.10:8080 Important fields: protocol accepts HTTP, HTTPS, SOCKS4, or SOCKS5. credentials can hold user/password pairs. The url property encodes them automatically for client libraries like httpx or requests . max_concurrency limits simultaneous leases. None means unlimited. current_leases is managed by the storage adapter; treat it as read-only.","title":"Proxy"},{"location":"models/#proxypool","text":"Pools group proxies with shared configuration. Use pool names to pick distinct provider buckets or geographic segments. from lighthouse import ProxyPool pool = ProxyPool(name=\"latam-residential\", description=\"Residential LatAm proxies\")","title":"ProxyPool"},{"location":"models/#consumers","text":"A Consumer identifies the actor leasing proxies. This can be a worker name, a service, or any identifier meaningful to your system. from lighthouse import Consumer storage.add_consumer(Consumer(name=\"worker-1\"))","title":"Consumers"},{"location":"models/#leases","text":"Lease records who is using a proxy and when the access expires. You usually interact with this through ProxyManager.acquire_proxy , but the data model is available if you need to persist or audit leases. lease.id lease.proxy_id lease.expires_at","title":"Leases"},{"location":"models/#proxy-filters","text":"ProxyFilters lets you express selection criteria. All fields are optional, and adapters decide how to evaluate them. from lighthouse import ProxyFilters filters = ProxyFilters( country=\"CO\", source=\"latam-provider\", asn=12345, latitude=4.7110, longitude=-74.0721, radius_km=50, ) Validation rules: Latitude and longitude must appear together. If radius_km is set, geolocation coordinates are required.","title":"Proxy Filters"},{"location":"models/#status-enums","text":"Both proxies and leases expose enums for their lifecycle. from lighthouse import ProxyStatus, LeaseStatus if result.status is ProxyStatus.SLOW: ... if lease.status is LeaseStatus.RELEASED: ... Stay consistent with these enums when writing storage adapters or APIs so that all Lighthouse components can reason about state transitions identically.","title":"Status Enums"},{"location":"proxy-manager/","text":"Proxy Manager \u00b6 ProxyManager is the high-level orchestration API for leasing and releasing proxies. It uses a storage backend that implements the IStorage interface to keep track of pools, proxies, consumers, and leases. Creating a Manager \u00b6 from lighthouse import InMemoryStorage, ProxyManager storage = InMemoryStorage() manager = ProxyManager(storage) The manager itself is stateless; all persistent data lives in the storage backend. You can use the bundled in-memory adapter for tests and scripts, or wire your own adapter for production. Leasing a Proxy \u00b6 lease = manager.acquire_proxy( pool_name=\"latam-residential\", consumer_name=\"worker-1\", duration_seconds=300, ) if lease: print(\"Leased proxy\", lease.proxy_id) else: print(\"No proxy available\") Key points: If consumer_name is omitted, the manager falls back to the default consumer ( ProxyManager.DEFAULT_CONSUMER_NAME ). Make sure that consumer exists in storage. duration_seconds defines when the lease expires. The storage adapter is responsible for releasing expired leases. The manager automatically calls cleanup_expired_leases() before trying to allocate a proxy, so stale leases do not block new requests. Releasing a Proxy \u00b6 if lease: manager.release_proxy(lease) Leases should be released as soon as the caller finishes using the proxy. The storage layer decrements current_leases and updates the lease status. Filtering Proxies \u00b6 Use ProxyFilters to target specific proxies. Filters apply metadata such as country, provider, or geolocation: from lighthouse import ProxyFilters filters = ProxyFilters(country=\"AR\", source=\"oxylabs\") lease = manager.acquire_proxy( pool_name=\"latam-residential\", consumer_name=\"worker-1\", filters=filters, ) If you need radius-based matching, include latitude , longitude , and radius_km . Storage adapters are in charge of interpreting these filters. Handling Concurrency Limits \u00b6 Each Proxy can define max_concurrency . The storage implementation checks the current lease count and prevents over-leasing. from lighthouse import Proxy, ProxyStatus proxy = Proxy( host=\"1.1.1.1\", port=8080, protocol=\"http\", pool_id=pool.id, status=ProxyStatus.ACTIVE, max_concurrency=2, ) If all slots are in use, acquire_proxy returns None and the caller can retry or pick another pool. Cleaning Up Expired Leases \u00b6 You can trigger cleanup manually when running background jobs: released = manager.cleanup_expired_leases() print(\"Expired leases released:\", released) Well-behaved storage adapters should also perform cleanup on their own cadence (e.g., cron job, background task, or database job).","title":"Proxy Manager"},{"location":"proxy-manager/#proxy-manager","text":"ProxyManager is the high-level orchestration API for leasing and releasing proxies. It uses a storage backend that implements the IStorage interface to keep track of pools, proxies, consumers, and leases.","title":"Proxy Manager"},{"location":"proxy-manager/#creating-a-manager","text":"from lighthouse import InMemoryStorage, ProxyManager storage = InMemoryStorage() manager = ProxyManager(storage) The manager itself is stateless; all persistent data lives in the storage backend. You can use the bundled in-memory adapter for tests and scripts, or wire your own adapter for production.","title":"Creating a Manager"},{"location":"proxy-manager/#leasing-a-proxy","text":"lease = manager.acquire_proxy( pool_name=\"latam-residential\", consumer_name=\"worker-1\", duration_seconds=300, ) if lease: print(\"Leased proxy\", lease.proxy_id) else: print(\"No proxy available\") Key points: If consumer_name is omitted, the manager falls back to the default consumer ( ProxyManager.DEFAULT_CONSUMER_NAME ). Make sure that consumer exists in storage. duration_seconds defines when the lease expires. The storage adapter is responsible for releasing expired leases. The manager automatically calls cleanup_expired_leases() before trying to allocate a proxy, so stale leases do not block new requests.","title":"Leasing a Proxy"},{"location":"proxy-manager/#releasing-a-proxy","text":"if lease: manager.release_proxy(lease) Leases should be released as soon as the caller finishes using the proxy. The storage layer decrements current_leases and updates the lease status.","title":"Releasing a Proxy"},{"location":"proxy-manager/#filtering-proxies","text":"Use ProxyFilters to target specific proxies. Filters apply metadata such as country, provider, or geolocation: from lighthouse import ProxyFilters filters = ProxyFilters(country=\"AR\", source=\"oxylabs\") lease = manager.acquire_proxy( pool_name=\"latam-residential\", consumer_name=\"worker-1\", filters=filters, ) If you need radius-based matching, include latitude , longitude , and radius_km . Storage adapters are in charge of interpreting these filters.","title":"Filtering Proxies"},{"location":"proxy-manager/#handling-concurrency-limits","text":"Each Proxy can define max_concurrency . The storage implementation checks the current lease count and prevents over-leasing. from lighthouse import Proxy, ProxyStatus proxy = Proxy( host=\"1.1.1.1\", port=8080, protocol=\"http\", pool_id=pool.id, status=ProxyStatus.ACTIVE, max_concurrency=2, ) If all slots are in use, acquire_proxy returns None and the caller can retry or pick another pool.","title":"Handling Concurrency Limits"},{"location":"proxy-manager/#cleaning-up-expired-leases","text":"You can trigger cleanup manually when running background jobs: released = manager.cleanup_expired_leases() print(\"Expired leases released:\", released) Well-behaved storage adapters should also perform cleanup on their own cadence (e.g., cron job, background task, or database job).","title":"Cleaning Up Expired Leases"},{"location":"storage/","text":"Storage Adapters \u00b6 The toolkit separates business rules from persistence using the IStorage interface. You can plug in custom adapters for your database while reusing the same acquisition logic across applications. In-Memory Storage \u00b6 The bundled InMemoryStorage is ideal for tests and exploratory scripts. from lighthouse import ( Consumer, InMemoryStorage, Proxy, ProxyPool, ProxyStatus, ) storage = InMemoryStorage() pool = ProxyPool(name=\"latam-residential\") storage.add_pool(pool) storage.add_consumer(Consumer(name=\"default\")) storage.add_proxy( Proxy( host=\"186.33.123.10\", port=8080, protocol=\"http\", pool_id=pool.id, status=ProxyStatus.ACTIVE, ) ) Characteristics: Thread-safe through an internal RLock . Keeps data structures in Python dictionaries; nothing is persisted to disk. Provides helper methods ( add_pool , add_proxy , add_consumer ) to seed test data. Production adapters can expose similar helpers or rely on migrations. Implementing IStorage \u00b6 Custom adapters live in your service or SDK codebase. They must implement: find_available_proxy(pool_name, filters) create_lease(proxy, consumer_name, duration_seconds) release_lease(lease) cleanup_expired_leases() Typical responsibilities include: Translating ProxyFilters into database queries. Enforcing max_concurrency when creating leases. Persisting lease state changes and adjusting current_leases counters. Returning defensive copies of models so callers cannot mutate shared state. You can extend the models with extra fields (e.g., tags , datacenter ) as long as they round-trip through the adapter and the additional metadata remains optional for other consumers. Planning Additional Adapters \u00b6 Future storage modules might target: Relational databases (PostgreSQL, MySQL) using SQLAlchemy or async drivers. Document stores (MongoDB) for flexible metadata. Distributed caches (Redis) when leases need to be tracked at high volume. Keep the adapter project-specific so the toolkit remains storage-agnostic. When multiple teams need the same adapter, consider publishing it as a separate package that depends on lighthouse .","title":"Storage Adapters"},{"location":"storage/#storage-adapters","text":"The toolkit separates business rules from persistence using the IStorage interface. You can plug in custom adapters for your database while reusing the same acquisition logic across applications.","title":"Storage Adapters"},{"location":"storage/#in-memory-storage","text":"The bundled InMemoryStorage is ideal for tests and exploratory scripts. from lighthouse import ( Consumer, InMemoryStorage, Proxy, ProxyPool, ProxyStatus, ) storage = InMemoryStorage() pool = ProxyPool(name=\"latam-residential\") storage.add_pool(pool) storage.add_consumer(Consumer(name=\"default\")) storage.add_proxy( Proxy( host=\"186.33.123.10\", port=8080, protocol=\"http\", pool_id=pool.id, status=ProxyStatus.ACTIVE, ) ) Characteristics: Thread-safe through an internal RLock . Keeps data structures in Python dictionaries; nothing is persisted to disk. Provides helper methods ( add_pool , add_proxy , add_consumer ) to seed test data. Production adapters can expose similar helpers or rely on migrations.","title":"In-Memory Storage"},{"location":"storage/#implementing-istorage","text":"Custom adapters live in your service or SDK codebase. They must implement: find_available_proxy(pool_name, filters) create_lease(proxy, consumer_name, duration_seconds) release_lease(lease) cleanup_expired_leases() Typical responsibilities include: Translating ProxyFilters into database queries. Enforcing max_concurrency when creating leases. Persisting lease state changes and adjusting current_leases counters. Returning defensive copies of models so callers cannot mutate shared state. You can extend the models with extra fields (e.g., tags , datacenter ) as long as they round-trip through the adapter and the additional metadata remains optional for other consumers.","title":"Implementing IStorage"},{"location":"storage/#planning-additional-adapters","text":"Future storage modules might target: Relational databases (PostgreSQL, MySQL) using SQLAlchemy or async drivers. Document stores (MongoDB) for flexible metadata. Distributed caches (Redis) when leases need to be tracked at high volume. Keep the adapter project-specific so the toolkit remains storage-agnostic. When multiple teams need the same adapter, consider publishing it as a separate package that depends on lighthouse .","title":"Planning Additional Adapters"},{"location":"theme/","text":"Documentation Theme (mkdocs-shadcn) \u00b6 The Lighthouse docs use the mkdocs-shadcn theme. It applies the shadcn/ui design system on top of MkDocs so the site feels modern without custom CSS. Quick Start \u00b6 Install the theme alongside MkDocs: pip install mkdocs mkdocs-shadcn Then enable it in mkdocs.yml : theme: name: shadcn Supported Extensions \u00b6 The theme works with the standard Markdown extensions plus several pymdownx helpers. Lighthouse currently enables: admonition tables toc codehilite pymdownx.details pymdownx.highlight pymdownx.inlinehilite pymdownx.superfences pymdownx.tabbed Syntax colors come from the theme\u2019s bundled Pygments palette ( github-dark ), configured through the pygments_style setting in mkdocs.yml . Additional extensions you can enable: pymdownx.blocks.details pymdownx.blocks.tab pymdownx.progressbar pymdownx.arithmatex built-in shadcn.echarts , shadcn.iconify , shadcn.codexec Plugins \u00b6 built-in excalidraw \u2013 edit diagrams in dev mode and render SVG at build time. mkdocstrings \u2013 auto-generate API docs from docstrings (experimental in the shadcn theme). These plugins are optional. Enable them in mkdocs.yml when you need the functionality. Developing the Theme \u00b6 The upstream project exposes its Tailwind CSS source for contributors. To work on the theme itself, clone the repository and install both the Python and CSS prerequisites: git clone https://github.com/asiffer/mkdocs-shadcn cd mkdocs-shadcn uv sync --all-extras bun install # or npm/yarn/pnpm Run the example docs in watch mode while hacking on the theme: cd pages/ uv run mkdocs serve --watch-theme -w .. In another terminal, keep the Tailwind watcher running from the project root: bun dev The Lighthouse repository does not need Tailwind or the dev tooling\u2014only the published mkdocs-shadcn package is required.","title":"Documentation Theme (mkdocs-shadcn)"},{"location":"theme/#documentation-theme-mkdocs-shadcn","text":"The Lighthouse docs use the mkdocs-shadcn theme. It applies the shadcn/ui design system on top of MkDocs so the site feels modern without custom CSS.","title":"Documentation Theme (mkdocs-shadcn)"},{"location":"theme/#quick-start","text":"Install the theme alongside MkDocs: pip install mkdocs mkdocs-shadcn Then enable it in mkdocs.yml : theme: name: shadcn","title":"Quick Start"},{"location":"theme/#supported-extensions","text":"The theme works with the standard Markdown extensions plus several pymdownx helpers. Lighthouse currently enables: admonition tables toc codehilite pymdownx.details pymdownx.highlight pymdownx.inlinehilite pymdownx.superfences pymdownx.tabbed Syntax colors come from the theme\u2019s bundled Pygments palette ( github-dark ), configured through the pygments_style setting in mkdocs.yml . Additional extensions you can enable: pymdownx.blocks.details pymdownx.blocks.tab pymdownx.progressbar pymdownx.arithmatex built-in shadcn.echarts , shadcn.iconify , shadcn.codexec","title":"Supported Extensions"},{"location":"theme/#plugins","text":"built-in excalidraw \u2013 edit diagrams in dev mode and render SVG at build time. mkdocstrings \u2013 auto-generate API docs from docstrings (experimental in the shadcn theme). These plugins are optional. Enable them in mkdocs.yml when you need the functionality.","title":"Plugins"},{"location":"theme/#developing-the-theme","text":"The upstream project exposes its Tailwind CSS source for contributors. To work on the theme itself, clone the repository and install both the Python and CSS prerequisites: git clone https://github.com/asiffer/mkdocs-shadcn cd mkdocs-shadcn uv sync --all-extras bun install # or npm/yarn/pnpm Run the example docs in watch mode while hacking on the theme: cd pages/ uv run mkdocs serve --watch-theme -w .. In another terminal, keep the Tailwind watcher running from the project root: bun dev The Lighthouse repository does not need Tailwind or the dev tooling\u2014only the published mkdocs-shadcn package is required.","title":"Developing the Theme"}]}